{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNvKfIxoDpBOQMfQ4nZvNsn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anirudhgudi/quad-sdk-Unitree_Go2/blob/main/quadruped_go2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0IyGWT99YYq",
        "outputId": "13bfd80d-847b-4f23-eb21-7ce574f331a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mujoco in /usr/local/lib/python3.12/dist-packages (3.3.7)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mujoco) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.12/dist-packages (from mujoco) (1.13.0)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.12/dist-packages (from mujoco) (2.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mujoco) (2.0.2)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.12/dist-packages (from mujoco) (3.1.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (6.5.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (4.15.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (3.23.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install mujoco\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy gymnasium mujoco scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfkBd-SU9sqn",
        "outputId": "253bc68b-adc9-41af-bde5-aaec6e7d19cc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: mujoco in /usr/local/lib/python3.12/dist-packages (3.3.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mujoco) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.12/dist-packages (from mujoco) (1.13.0)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.12/dist-packages (from mujoco) (2.10.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.12/dist-packages (from mujoco) (3.1.10)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "ztCXR6qA9_9y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" # @title\n",
        "{\n",
        " \"cells\": [\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"# Go2 Quadruped RL Trainer (MuJoCo + PPO)\\n\",\n",
        "    \"\\n\",\n",
        "    \"This notebook will set up a complete environment for training the Unitree Go2 robot to walk using Reinforcement Learning (PPO). \\n\",\n",
        "    \"\\n\",\n",
        "    \"**Instructions:**\\n\",\n",
        "    \"1.  Make sure your runtime is set to use a GPU (Runtime > Change runtime type > T4 GPU) for faster training.\\n\",\n",
        "    \"2.  Run the cells in order from top to bottom.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"## Step 1: Install Dependencies\\n\",\n",
        "    \"\\n\",\n",
        "    \"This cell installs all necessary Python libraries for the simulation and RL algorithm.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"!pip install -q mujoco gymnasium torch scipy\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"## Step 2: Get Robot Model and Assets\\n\",\n",
        "    \"\\n\",\n",
        "    \"We clone the official `unitree_mujoco` repository. This gives us the `go2.xml` file and, most importantly, the `assets/` folder containing all the `.obj` mesh files the robot model needs to load.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"!git clone https://github.com/unitreerobotics/unitree_mujoco\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"## Step 3: Create Python Training Files\\n\",\n",
        "    \"\\n\",\n",
        "    \"We use the `%%writefile` magic command to create our four Python scripts in the Colab environment's main directory.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"%%writefile config.py\\n\",\n",
        "    \"import numpy as np\\n\",\n",
        "    \"\\n\",\n",
        "    \"# --- Simulation ---\\n\",\n",
        "    \"# This path now points inside the cloned repository\\n\",\n",
        "    \"XML_PATH = 'unitree_mujoco/unitree_robots/go2/go2.xml'\\n\",\n",
        "    \"\\n\",\n",
        "    \"SIM_HZ = 500  # (Hz) Frequency of MuJoCo physics steps\\n\",\n",
        "    \"CONTROL_HZ = 50  # (Hz) Frequency of RL agent policy decisions\\n\",\n",
        "    \"FRAME_SKIP = SIM_HZ // CONTROL_HZ # Number of physics steps per agent step\\n\",\n",
        "    \"MAX_EPISODE_STEPS = 1000  # Max steps before env reset\\n\",\n",
        "    \"\\n\",\n",
        "    \"# --- Robot Model ---\\n\",\n",
        "    \"# These names are taken from the official 'go2.xml'\\n\",\n",
        "    \"TRUNK_BODY_NAME = \\\"trunk\\\"\\n\",\n",
        "    \"JOINT_NAMES = [\\n\",\n",
        "    \"    \\\"FR_hip_joint\\\", \\\"FR_thigh_joint\\\", \\\"FR_calf_joint\\\",\\n\",\n",
        "    \"    \\\"FL_hip_joint\\\", \\\"FL_thigh_joint\\\", \\\"FL_calf_joint\\\",\\n\",\n",
        "    \"    \\\"RR_hip_joint\\\", \\\"RR_thigh_joint\\\", \\\"RR_calf_joint\\\",\\n\",\n",
        "    \"    \\\"RL_hip_joint\\\", \\\"RL_thigh_joint\\\", \\\"RL_calf_joint\\\",\\n\",\n",
        "    \"]\\n\",\n",
        "    \"# We use the calf bodies to check for contact, as they contain the foot geoms\\n\",\n",
        "    \"FOOT_BODY_NAMES = [\\\"FR_calf\\\", \\\"FL_calf\\\", \\\"RR_calf\\\", \\\"RL_calf\\\"]\\n\",\n",
        "    \"\\n\",\n",
        "    \"# --- Locomotion ---\\n\",\n",
        "    \"TARGET_VELOCITY = 0.8  # (m/s) Target forward velocity (x-axis)\\n\",\n",
        "    \"TARGET_HEIGHT = 0.3    # (m) Target CoM height\\n\",\n",
        "    \"CONTACT_FORCE_THRESHOLD = 5.0  # (N) Force threshold to register foot contact\\n\",\n",
        "    \"\\n\",\n",
        "    \"# --- RL Reward Weights ---\\n\",\n",
        "    \"W_VEL_X = 2.0         # Reward for matching target x-velocity\\n\",\n",
        "    \"W_VEL_Y = -1.0        # Penalty for y-velocity\\n\",\n",
        "    \"W_VEL_Z = -1.0        # Penalty for z-velocity\\n\",\n",
        "    \"W_ANG_VEL = -0.1      # Penalty for angular velocity\\n\",\n",
        "    \"W_COM_HEIGHT = 1.5    # Reward for maintaining target height\\n\",\n",
        "    \"W_ORIENTATION = -2.0  # Penalty for roll and pitch\\n\",\n",
        "    \"W_ACTION_RATE = -0.01 # Penalty for jerky actions\\n\",\n",
        "    \"W_TORQUE = -0.00002   # Penalty for motor effort (torques)\\n\",\n",
        "    \"W_CONTACT_FORCE = -0.0001 # Penalty for high contact forces\\n\",\n",
        "    \"W_COM_IN_SUPPORT = 3.0 # Reward for keeping CoM in support polygon\\n\",\n",
        "    \"W_FALL = -200.0       # Large penalty for falling\\n\",\n",
        "    \"\\n\",\n",
        "    \"# --- PPO Training ---\\n\",\n",
        "    \"PPO_STEPS_PER_EPOCH = 4096\\n\",\n",
        "    \"PPO_EPOCHS = 500\\n\",\n",
        "    \"PPO_LEARNING_RATE = 3e-4\\n\",\n",
        "    \"PPO_MINIBATCH_SIZE = 64\\n\",\n",
        "    \"PPO_UPDATE_EPOCHS = 10\\n\",\n",
        "    \"PPO_GAMMA = 0.99\\n\",\n",
        "    \"PPO_LAM = 0.95\\n\",\n",
        "    \"PPO_CLIP = 0.2\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"%%writefile utils.py\\n\",\n",
        "    \"import numpy as np\\n\",\n",
        "    \"import mujoco\\n\",\n",
        "    \"from scipy.spatial import ConvexHull, Delaunay\\n\",\n",
        "    \"\\n\",\n",
        "    \"# --- MuJoCo Model/Data Getters ---\\n\",\n",
        "    \"\\n\",\n",
        "    \"def get_body_id(model, body_name):\\n\",\n",
        "    \"    \\\"\\\"\\\"Returns the MuJoCo ID for a body.\\\"\\\"\\\"\\n\",\n",
        "    \"    return mujoco.mj_name2id(model, mujoco.mjtObj.mjOBJ_BODY, body_name)\\n\",\n",
        "    \"\\n\",\n",
        "    \"def get_joint_qpos_ids(model, joint_names):\\n\",\n",
        "    \"    \\\"\\\"\\\"Returns qpos indices for a list of joint names.\\\"\\\"\\\"\\n\",\n",
        "    \"    return [model.jnt_qposadr[mujoco.mj_name2id(model, mujoco.mjtObj.mjOBJ_JOINT, name)] for name in joint_names]\\n\",\n",
        "    \"\\n\",\n",
        "    \"def get_joint_qvel_ids(model, joint_names):\\n\",\n",
        "    \"    \\\"\\\"\\\"Returns qvel indices for a list of joint names.\\\"\\\"\\\"\\n\",\n",
        "    \"    return [model.jnt_dofadr[mujoco.mj_name2id(model, mujoco.mjtObj.mjOBJ_JOINT, name)] for name in joint_names]\\n\",\n",
        "    \"\\n\",\n",
        "    \"def get_actuator_ids(model, joint_names):\\n\",\n",
        "    \"    \\\"\\\"\\\"Returns actuator indices for a list of joint names.\\\"\\\"\\\"\\n\",\n",
        "    \"    # Assumes motor name is joint name + \\\"_motor\\\"\\n\",\n",
        "    \"    return [mujoco.mj_name2id(model, mujoco.mjtObj.mjOBJ_ACTUATOR, name.replace(\\\"_joint\\\", \\\"_motor\\\")) for name in joint_names]\\n\",\n",
        "    \"\\n\",\n",
        "    \"def get_foot_body_ids(model, foot_body_names):\\n\",\n",
        "    \"    \\\"\\\"\\\"Returns body IDs for a list of foot body names.\\\"\\\"\\\"\\n\",\n",
        "    \"    return [get_body_id(model, name) for name in foot_body_names]\\n\",\n",
        "    \"\\n\",\n",
        "    \"# --- Kinematics & Dynamics ---\\n\",\n",
        "    \"\\n\",\n",
        "    \"def get_com_position(data, trunk_id):\\n\",\n",
        "    \"    \\\"\\\"\\\"Returns the 3D position of the trunk (CoM).\\\"\\\"\\\"\\n\",\n",
        "    \"    return data.xpos[trunk_id]\\n\",\n",
        "    \"\\n\",\n",
        "    \"def get_com_velocity(data, trunk_id):\\n\",\n",
        "    \"    \\\"\\\"\\\"Returns the 3D linear velocity of the trunk (CoM).\\\"\\\"\\\"\\n\",\n",
        "    \"    # Use cvel (velocities in world frame)\\n\",\n",
        "    \"    return data.cvel[trunk_id, 3:6]\\n\",\n",
        "    \"\\n\",\n",
        "    \"def get_body_orientation(data, trunk_id):\\n\",\n",
        "    \"    \\\"\\\"\\\"Returns the quaternion orientation of the trunk.\\\"\\\"\\\"\\n\",\n",
        "    \"    return data.xquat[trunk_id]\\n\",\n",
        "    \"\\n\",\n",
        "    \"def get_body_angular_velocity(data, trunk_id):\\n\",\n",
        "    \"    \\\"\\\"\\\"Returns the 3D angular velocity of the trunk.\\\"\\\"\\\"\\n\",\n",
        "    \"    # Use cvel (velocities in world frame)\\n\",\n",
        "    \"    return data.cvel[trunk_id, 0:3]\\n\",\n",
        "    \"\\n\",\n",
        "    \"def quat_to_rpy(quat):\\n\",\n",
        "    \"    \\\"\\\"\\\"Converts a quaternion (w, x, y, z) to roll, pitch, yaw.\\\"\\\"\\\"\\n\",\n",
        "    \"    w, x, y, z = quat\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    # Roll (x-axis rotation)\\n\",\n",
        "    \"    sinr_cosp = 2 * (w * x + y * z)\\n\",\n",
        "    \"    cosr_cosp = 1 - 2 * (x * x + y * y)\\n\",\n",
        "    \"    roll = np.arctan2(sinr_cosp, cosr_cosp)\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    # Pitch (y-axis rotation)\\n\",\n",
        "    \"    sinp = 2 * (w * y - z * x)\\n\",\n",
        "    \"    if np.abs(sinp) >= 1:\\n\",\n",
        "    \"        pitch = np.copysign(np.pi / 2, sinp)  # Use 90 degrees if out of range\\n\",\n",
        "    \"    else:\\n\",\n",
        "    \"        pitch = np.arcsin(sinp)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"    # Yaw (z-axis rotation)\\n\",\n",
        "    \"    siny_cosp = 2 * (w * z + x * y)\\n\",\n",
        "    \"    cosy_cosp = 1 - 2 * (y * y + z * z)\\n\",\n",
        "    \"    yaw = np.arctan2(siny_cosp, cosy_cosp)\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    return roll, pitch, yaw\\n\",\n",
        "    \"\\n\",\n",
        "    \"# --- Contact & Stability ---\\n\",\n",
        "    \"\\n\",\n",
        "    \"def get_foot_contacts(model, data, foot_body_ids, contact_force_threshold):\\n\",\n",
        "    \"    \\\"\\\"\\\"\\n\",\n",
        "    \"    Checks for foot contact with the ground.\\n\",\n",
        "    \"    Returns a boolean array [FR, FL, RR, RL]\\n\",\n",
        "    \"    \\\"\\\"\\\"\\n\",\n",
        "    \"    contacts = [False] * 4\\n\",\n",
        "    \"    for i in range(data.ncon):\\n\",\n",
        "    \"        contact = data.contact[i]\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # Check if geom1 or geom2 is a foot\\n\",\n",
        "    \"        geom1_body = model.geom_bodyid[contact.geom1]\\n\",\n",
        "    \"        geom2_body = model.geom_bodyid[contact.geom2]\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        is_geom1_foot = geom1_body in foot_body_ids\\n\",\n",
        "    \"        is_geom2_foot = geom2_body in foot_body_ids\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        if not (is_geom1_foot or is_geom2_foot):\\n\",\n",
        "    \"            continue # Not a foot contact\\n\",\n",
        "    \"\\n\",\n",
        "    \"        # Check if the other geom is the ground (geom ID 0)\\n\",\n",
        "    \"        is_geom1_ground = contact.geom1 == 0\\n\",\n",
        "    \"        is_geom2_ground = contact.geom2 == 0\\n\",\n",
        "    \"\\n\",\n",
        "    \"        if not (is_geom1_ground or is_geom2_ground):\\n\",\n",
        "    \"            continue # Not a ground contact\\n\",\n",
        "    \"            \\n\",\n",
        "    \"        # Get contact force\\n\",\n",
        "    \"        force_normal = np.zeros(3)\\n\",\n",
        "    \"        mujoco.mj_contactForce(model, data, i, force_normal)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        if np.linalg.norm(force_normal) > contact_force_threshold:\\n\",\n",
        "    \"            if is_geom1_foot:\\n\",\n",
        "    \"                foot_idx = foot_body_ids.index(geom1_body)\\n\",\n",
        "    \"                contacts[foot_idx] = True\\n\",\n",
        "    \"            if is_geom2_foot:\\n\",\n",
        "    \"                foot_idx = foot_body_ids.index(geom2_body)\\n\",\n",
        "    \"                contacts[foot_idx] = True\\n\",\n",
        "    \"                \\n\",\n",
        "    \"    return np.array(contacts)\\n\",\n",
        "    \"\\n\",\n",
        "    \"def get_foot_positions(data, foot_body_ids):\\n\",\n",
        "    \"    \\\"\\\"\\\"Returns the 3D world positions of the feet (calf bodies).\\\"\\\"\\\"\\n\",\n",
        "    \"    return data.xpos[foot_body_ids]\\n\",\n",
        "    \"\\n\",\n",
        "    \"def get_support_polygon(foot_positions, contact_states):\\n\",\n",
        "    \"    \\\"\\\"\\\"\\n\",\n",
        "    \"    Returns the 2D vertices (x, y) of the support polygon.\\n\",\n",
        "    \"    Returns an empty list if fewer than 2 feet are in contact.\\n\",\n",
        "    \"    \\\"\\\"\\\"\\n\",\n",
        "    \"    stance_feet_pos = foot_positions[contact_states, :2] # Get (x, y) of stance feet\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    if stance_feet_pos.shape[0] < 2:\\n\",\n",
        "    \"        return [] # Not enough points to form a polygon\\n\",\n",
        "    \"        \\n\",\n",
        "    \"    if stance_feet_pos.shape[0] == 2:\\n\",\n",
        "    \"        return stance_feet_pos # Support polygon is a line\\n\",\n",
        "    \"        \\n\",\n",
        "    \"    try:\\n\",\n",
        "    \"        # A Bounding Box is simpler and more stable than Convex Hull for 3 points\\n\",\n",
        "    \"        if stance_feet_pos.shape[0] == 3:\\n\",\n",
        "    \"             return stance_feet_pos\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        hull = ConvexHull(stance_feet_pos)\\n\",\n",
        "    \"        return stance_feet_pos[hull.vertices]\\n\",\n",
        "    \"    except Exception:\\n\",\n",
        "    \"        return [] # Error during hull calculation (e.g., colinear points)\\n\",\n",
        "    \"\\n\",\n",
        "    \"def is_com_stable(com_pos_2d, support_polygon):\\n\",\n",
        "    \"    \\\"\\\"\\\"\\n\",\n",
        "    \"    Checks if the 2D CoM position is inside the 2D support polygon.\\n\",\n",
        "    \"    Uses scipy.spatial.Delaunay for robust point-in-polygon check.\\n\",\n",
        "    \"    \\\"\\\"\\\"\\n\",\n",
        "    \"    if len(support_polygon) < 3:\\n\",\n",
        "    \"        # If support is a line (2 feet) or point (1 foot),\\n\",\n",
        "    \"        # we can't use a polygon check.\\n\",\n",
        "    \"        # For simplicity, we'll call it \\\"unstable\\\"\\n\",\n",
        "    \"        return False\\n\",\n",
        "    \"        \\n\",\n",
        "    \"    try:\\n\",\n",
        "    \"        # Create a Delaunay triangulation of the support polygon\\n\",\n",
        "    \"        hull = Delaunay(support_polygon)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # find_simplex returns -1 if the point is outside the hull\\n\",\n",
        "    \"        return hull.find_simplex(com_pos_2d) >= 0\\n\",\n",
        "    \"    except Exception:\\n\",\n",
        "    \"        # Error (e.g., flat polygon)\\n\",\n",
        "    \"        return False\\n\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"%%writefile go2_env.py\\n\",\n",
        "    \"import mujoco\\n\",\n",
        "    \"import gymnasium as gym\\n\",\n",
        "    \"from gymnasium import spaces\\n\",\n",
        "    \"import numpy as np\\n\",\n",
        "    \"import os\\n\",\n",
        "    \"\\n\",\n",
        "    \"import config\\n\",\n",
        "    \"import utils\\n\",\n",
        "    \"\\n\",\n",
        "    \"class Go2Env(gym.Env):\\n\",\n",
        "    \"    \\\"\\\"\\\"\\n\",\n",
        "    \"    Custom Gymnasium environment for the Unitree Go2 robot using MuJoCo.\\n\",\n",
        "    \"    \\\"\\\"\\\"\\n\",\n",
        "    \"    metadata = {\\\"render_modes\\\": [\\\"human\\\", \\\"rgb_array\\\"], \\\"render_fps\\\": 50}\\n\",\n",
        "    \"\\n\",\n",
        "    \"    def __init__(self, render_mode=None):\\n\",\n",
        "    \"        super().__init__()\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        self.render_mode = render_mode\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # Construct the full path to the XML file\\n\",\n",
        "    \"        # This path is relative to where the script is run (e.g., /content/)\\n\",\n",
        "    \"        xml_path = config.XML_PATH\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        if not os.path.exists(xml_path):\\n\",\n",
        "    \"            raise FileNotFoundError(\\n\",\n",
        "    \"                f\\\"Could not find XML file: {xml_path}. \\\"\\n\",\n",
        "    \"                f\\\"Make sure the 'unitree_mujoco' repo was cloned successfully.\\\"\\n\",\n",
        "    \"            )\\n\",\n",
        "    \"            \\n\",\n",
        "    \"        self.model = mujoco.MjModel.from_xml_path(xml_path)\\n\",\n",
        "    \"        self.data = mujoco.MjData(self.model)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # --- Get element IDs from model ---\\n\",\n",
        "    \"        self.trunk_id = utils.get_body_id(self.model, config.TRUNK_BODY_NAME)\\n\",\n",
        "    \"        self.joint_qpos_ids = utils.get_joint_qpos_ids(self.model, config.JOINT_NAMES)\\n\",\n",
        "    \"        self.joint_qvel_ids = utils.get_joint_qvel_ids(self.model, config.JOINT_NAMES)\\n\",\n",
        "    \"        self.actuator_ids = utils.get_actuator_ids(self.model, config.JOINT_NAMES)\\n\",\n",
        "    \"        self.foot_body_ids = utils.get_foot_body_ids(self.model, config.FOOT_BODY_NAMES)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # Store initial state for resets\\n\",\n",
        "    \"        self.init_qpos = self.data.qpos.copy()\\n\",\n",
        "    \"        self.init_qvel = self.data.qvel.copy()\\n\",\n",
        "    \"        self.action_history = np.zeros(12)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # --- Define Action Space ---\\n\",\n",
        "    \"        act_dim = 12\\n\",\n",
        "    \"        ctrl_range = self.model.actuator_ctrlrange[self.actuator_ids]\\n\",\n",
        "    \"        self.action_low = ctrl_range[:, 0]\\n\",\n",
        "    \"        self.action_high = ctrl_range[:, 1]\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        self.action_space = spaces.Box(\\n\",\n",
        "    \"            low=self.action_low,\\n\",\n",
        "    \"            high=self.action_high,\\n\",\n",
        "    \"            dtype=np.float32\\n\",\n",
        "    \"        )\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # --- Define Observation Space ---\\n\",\n",
        "    \"        obs_dim = 3 + 3 + 4 + 12 + 12 + 4 # com_vel, ang_vel, quat, qpos, qvel, contacts\\n\",\n",
        "    \"        self.observation_space = spaces.Box(\\n\",\n",
        "    \"            low=-np.inf,\\n\",\n",
        "    \"            high=np.inf,\\n\",\n",
        "    \"            shape=(obs_dim,),\\n\",\n",
        "    \"            dtype=np.float64\\n\",\n",
        "    \"        )\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # --- Rendering (Colab) ---\\n\",\n",
        "    \"        # We'll use 'rgb_array' for Colab. 'human' mode won't work.\\n\",\n",
        "    \"        if self.render_mode == \\\"human\\\":\\n\",\n",
        "    \"            print(\\\"Warning: 'human' render_mode not supported in Colab. Use 'rgb_array' instead.\\\")\\n\",\n",
        "    \"            self.render_mode = \\\"rgb_array\\\"\\n\",\n",
        "    \"            \\n\",\n",
        "    \"        if self.render_mode == \\\"rgb_array\\\":\\n\",\n",
        "    \"            self.renderer = mujoco.Renderer(self.model, 480, 640)\\n\",\n",
        "    \"        else:\\n\",\n",
        "    \"            self.renderer = None\\n\",\n",
        "    \"\\n\",\n",
        "    \"    def _get_obs(self):\\n\",\n",
        "    \"        \\\"\\\"\\\"Constructs the observation vector from simulation data.\\\"\\\"\\\"\\n\",\n",
        "    \"        com_vel = utils.get_com_velocity(self.data, self.trunk_id)\\n\",\n",
        "    \"        ang_vel = utils.get_body_angular_velocity(self.data, self.trunk_id)\\n\",\n",
        "    \"        quat = utils.get_body_orientation(self.data, self.trunk_id)\\n\",\n",
        "    \"        qpos = self.data.qpos[self.joint_qpos_ids]\\n\",\n",
        "    \"        qvel = self.data.qvel[self.joint_qvel_ids]\\n\",\n",
        "    \"        contacts = utils.get_foot_contacts(\\n\",\n",
        "    \"            self.model, self.data, self.foot_body_ids, config.CONTACT_FORCE_THRESHOLD\\n\",\n",
        "    \"        )\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        return np.concatenate([\\n\",\n",
        "    \"            com_vel, ang_vel, quat, qpos, qvel, contacts.astype(float)\\n\",\n",
        "    \"        ])\\n\",\n",
        "    \"\\n\",\n",
        "    \"    def _compute_reward(self, action):\\n\",\n",
        "    \"        \\\"\\\"\\\"Calculates the reward based on the current state and action.\\\"\\\"\\\"\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # --- Get current state data ---\\n\",\n",
        "    \"        com_pos = utils.get_com_position(self.data, self.trunk_id)\\n\",\n",
        "    \"        com_vel = utils.get_com_velocity(self.data, self.trunk_id)\\n\",\n",
        "    \"        ang_vel = utils.get_body_angular_velocity(self.data, self.trunk_id)\\n\",\n",
        "    \"        quat = utils.get_body_orientation(self.data, self.trunk_id)\\n\",\n",
        "    \"        torques = self.data.actuator_force[self.actuator_ids]\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # 1. Velocity Tracking (X, Y, Z)\\n\",\n",
        "    \"        vel_error_x = (com_vel[0] - config.TARGET_VELOCITY)**2\\n\",\n",
        "    \"        vel_error_y = com_vel[1]**2\\n\",\n",
        "    \"        vel_error_z = com_vel[2]**2\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        reward_vel_x = config.W_VEL_X * np.exp(-vel_error_x * 5.0)\\n\",\n",
        "    \"        penalty_vel_y = config.W_VEL_Y * vel_error_y\\n\",\n",
        "    \"        penalty_vel_z = config.W_VEL_Z * vel_error_z\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # 2. CoM Height\\n\",\n",
        "    \"        height_error = (com_pos[2] - config.TARGET_HEIGHT)**2\\n\",\n",
        "    \"        reward_height = config.W_COM_HEIGHT * np.exp(-height_error * 20.0)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # 3. Orientation\\n\",\n",
        "    \"        roll, pitch, _ = utils.quat_to_rpy(quat)\\n\",\n",
        "    \"        orientation_penalty = config.W_ORIENTATION * (roll**2 + pitch**2)\\n\",\n",
        "    \"        ang_vel_penalty = config.W_ANG_VEL * np.sum(ang_vel**2)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # 4. Effort / Torque / Action Rate\\n\",\n",
        "    \"        torque_penalty = config.W_TORQUE * np.sum(torques**2)\\n\",\n",
        "    \"        action_rate_penalty = config.W_ACTION_RATE * np.sum((action - self.action_history)**2)\\n\",\n",
        "    \"        self.action_history = action # store for next step\\n\",\n",
        "    \"\\n\",\n",
        "    \"        # 5. Stability: CoM within Support Polygon\\n\",\n",
        "    \"        contacts = utils.get_foot_contacts(\\n\",\n",
        "    \"            self.model, self.data, self.foot_body_ids, config.CONTACT_FORCE_THRESHOLD\\n\",\n",
        "    \"        )\\n\",\n",
        "    \"        foot_positions = utils.get_foot_positions(self.data, self.foot_body_ids)\\n\",\n",
        "    \"        support_polygon = utils.get_support_polygon(foot_positions, contacts)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        is_stable = utils.is_com_stable(com_pos[:2], support_polygon)\\n\",\n",
        "    \"        reward_com_stable = config.W_COM_IN_SUPPORT if is_stable else 0.0\\n\",\n",
        "    \"\\n\",\n",
        "    \"        # --- Sum Rewards ---\\n\",\n",
        "    \"        total_reward = (\\n\",\n",
        "    \"            reward_vel_x + penalty_vel_y + penalty_vel_z +\\n\",\n",
        "    \"            reward_height + \\n\",\n",
        "    \"            orientation_penalty + ang_vel_penalty +\\n\",\n",
        "    \"            torque_penalty + action_rate_penalty +\\n\",\n",
        "    \"            reward_com_stable\\n\",\n",
        "    \"        )\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        reward_info = {\\n\",\n",
        "    \"            \\\"r_vel_x\\\": reward_vel_x,\\n\",\n",
        "    \"            \\\"p_vel_y\\\": penalty_vel_y,\\n\",\n",
        "    \"            \\\"p_vel_z\\\": penalty_vel_z,\\n\",\n",
        "    \"            \\\"r_height\\\": reward_height,\\n\",\n",
        "    \"            \\\"p_orientation\\\": orientation_penalty,\\n\",\n",
        "    \"            \\\"p_ang_vel\\\": ang_vel_penalty,\\n\",\n",
        "    \"            \\\"p_torque\\\": torque_penalty,\\n\",\n",
        "    \"            \\\"p_action_rate\\\": action_rate_penalty,\\n\",\n",
        "    \"            \\\"r_com_stable\\\": reward_com_stable,\\n\",\n",
        "    \"        }\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        return total_reward, reward_info\\n\",\n",
        "    \"\\n\",\n",
        "    \"    def _check_termination(self):\\n\",\n",
        "    \"        \\\"\\\"\\\"Checks if the episode should terminate (e.g., robot fell).\\\"\\\"\\\"\\n\",\n",
        "    \"        com_pos = utils.get_com_position(self.data, self.trunk_id)\\n\",\n",
        "    \"        quat = utils.get_body_orientation(self.data, self.trunk_id)\\n\",\n",
        "    \"        roll, pitch, _ = utils.quat_to_rpy(quat)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # Fell if CoM is too low or if roll/pitch is too high\\n\",\n",
        "    \"        is_fallen = (com_pos[2] < 0.15) or (abs(roll) > 1.0) or (abs(pitch) > 1.0)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        return is_fallen\\n\",\n",
        "    \"\\n\",\n",
        "    \"    def step(self, action):\\n\",\n",
        "    \"        \\\"\\\"\\\"Run one timestep of the environment's dynamics.\\\"\\\"\\\"\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # Clip action to be safe\\n\",\n",
        "    \"        action = np.clip(action, self.action_low, self.action_high)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # Set the target joint positions as the control signal\\n\",\n",
        "    \"        self.data.ctrl[self.actuator_ids] = action\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # Step the simulation forward\\n\",\n",
        "    \"        mujoco.mj_step(self.model, self.data, nstep=config.FRAME_SKIP)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # Get new state, reward, and termination status\\n\",\n",
        "    \"        observation = self._get_obs()\\n\",\n",
        "    \"        terminated = self._check_termination()\\n\",\n",
        "    \"        reward, reward_info = self._compute_reward(action)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        if terminated:\\n\",\n",
        "    \"            reward += config.W_FALL # Add large fall penalty\\n\",\n",
        "    \"\\n\",\n",
        "    \"        self.step_count += 1\\n\",\n",
        "    \"        truncated = self.step_count >= config.MAX_EPISODE_STEPS\\n\",\n",
        "    \"\\n\",\n",
        "    \"        return observation, reward, terminated, truncated, reward_info\\n\",\n",
        "    \"\\n\",\n",
        "    \"    def reset(self, seed=None):\\n\",\n",
        "    \"        \\\"\\\"\\\"Reset the environment to an initial state.\\\"\\\"\\\"\\n\",\n",
        "    \"        super().reset(seed=seed)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        self.step_count = 0\\n\",\n",
        "    \"        self.action_history = np.zeros(12)\\n\",\n",
        "    \"\\n\",\n",
        "    \"        mujoco.mj_resetData(self.model, self.data)\\n\",\n",
        "    \"        self.data.qpos[:] = self.init_qpos\\n\",\n",
        "    \"        self.data.qvel[:] = self.init_qvel\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # Add small random noise to initial joint positions\\n\",\n",
        "    \"        self.data.qpos[self.joint_qpos_ids] += self.np_random.uniform(\\n\",\n",
        "    \"            -0.1, 0.1, size=len(self.joint_qpos_ids)\\n\",\n",
        "    \"        )\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        mujoco.mj_forward(self.model, self.data)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        observation = self._get_obs()\\n\",\n",
        "    \"        return observation, {}\\n\",\n",
        "    \"\\n\",\n",
        "    \"    def render(self):\\n\",\n",
        "    \"        \\\"\\\"\\\"Render the environment (if in 'rgb_array' mode).\\\"\\\"\\\"\\n\",\n",
        "    \"        if self.renderer:\\n\",\n",
        "    \"            self.renderer.update_scene(self.data)\\n\",\n",
        "    \"            return self.renderer.render()\\n\",\n",
        "    \"        return None\\n\",\n",
        "    \"\\n\",\n",
        "    \"    def close(self):\\n\",\n",
        "    \"        \\\"\\\"\\\"Close the viewer.\\\"\\\"\\\"\\n\",\n",
        "    \"        if self.renderer:\\n\",\n",
        "    \"            self.renderer = None\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"%%writefile train_ppo.py\\n\",\n",
        "    \"import torch\\n\",\n",
        "    \"import torch.nn as nn\\n\",\n",
        "    \"import torch.optim as optim\\n\",\n",
        "    \"from torch.distributions.normal import Normal\\n\",\n",
        "    \"import numpy as np\\n\",\n",
        "    \"import time\\n\",\n",
        "    \"import os\\n\",\n",
        "    \"\\n\",\n",
        "    \"from go2_env import Go2Env  # Import the custom environment\\n\",\n",
        "    \"import config  # Import config file\\n\",\n",
        "    \"\\n\",\n",
        "    \"class ActorCritic(nn.Module):\\n\",\n",
        "    \"    \\\"\\\"\\\"PPO Actor-Critic network.\\\"\\\"\\\"\\n\",\n",
        "    \"    def __init__(self, obs_dim, action_dim, action_low, action_high):\\n\",\n",
        "    \"        super().__init__()\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # Store action scaling parameters\\n\",\n",
        "    \"        self.action_low = torch.tensor(action_low, dtype=torch.float32)\\n\",\n",
        "    \"        self.action_high = torch.tensor(action_high, dtype=torch.float32)\\n\",\n",
        "    \"        self.action_scale = (self.action_high - self.action_low) / 2.0\\n\",\n",
        "    \"        self.action_bias = (self.action_high + self.action_low) / 2.0\\n\",\n",
        "    \"\\n\",\n",
        "    \"        hidden_dim = 256\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # Critic network\\n\",\n",
        "    \"        self.critic = nn.Sequential(\\n\",\n",
        "    \"            nn.Linear(obs_dim, hidden_dim),\\n\",\n",
        "    \"            nn.ReLU(),\\n\",\n",
        "    \"            nn.Linear(hidden_dim, hidden_dim),\\n\",\n",
        "    \"            nn.ReLU(),\\n\",\n",
        "    \"            nn.Linear(hidden_dim, 1)\\n\",\n",
        "    \"        )\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # Actor network\\n\",\n",
        "    \"        self.actor = nn.Sequential(\\n\",\n",
        "    \"            nn.Linear(obs_dim, hidden_dim),\\n\",\n",
        "    \"            nn.ReLU(),\\n\",\n",
        "    \"            nn.Linear(hidden_dim, hidden_dim),\\n\",\n",
        "    \"            nn.ReLU(),\\n\",\n",
        "    \"            nn.Linear(hidden_dim, action_dim)\\n\",\n",
        "    \"        )\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # Standard deviation for the action distribution\\n\",\n",
        "    \"        self.actor_logstd = nn.Parameter(torch.zeros(1, action_dim))\\n\",\n",
        "    \"\\n\",\n",
        "    \"    def get_value(self, x):\\n\",\n",
        "    \"        return self.critic(x)\\n\",\n",
        "    \"\\n\",\n",
        "    \"    def get_action_and_value(self, x, action=None):\\n\",\n",
        "    \"        \\\"\\\"\\\"\\n\",\n",
        "    \"        Gets an action (and its log_prob) and the state value.\\n\",\n",
        "    \"        If action is provided, it evaluates that action.\\n\",\n",
        "    \"        If action is None, it samples a new action.\\n\",\n",
        "    \"        \\\"\\\"\\\"\\n\",\n",
        "    \"        # Actor output is the mean of a distribution in unbounded space\\n\",\n",
        "    \"        action_mean = self.actor(x)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        action_logstd = self.actor_logstd.expand_as(action_mean)\\n\",\n",
        "    \"        action_std = torch.exp(action_logstd)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        probs = Normal(action_mean, action_std)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        if action is None:\\n\",\n",
        "    \"            # Sample new action from the unbounded distribution\\n\",\n",
        "    \"            action_unbounded = probs.sample()\\n\",\n",
        "    \"            # Squash to [-1, 1] using Tanh\\n\",\n",
        "    \"            action_tanh = torch.tanh(action_unbounded)\\n\",\n",
        "    \"            # Scale and shift to the correct action range\\n\",\n",
        "    \"            action = self.action_bias + self.action_scale * action_tanh\\n\",\n",
        "    \"        else:\\n\",\n",
        "    \"            # Evaluate given action\\n\",\n",
        "    \"            # We need to reverse the scaling to get the \\\"tanh\\\" value\\n\",\n",
        "    \"            action_tanh = (action - self.action_bias) / self.action_scale\\n\",\n",
        "    \"            # Clip to avoid numerical issues at the bounds\\n\",\n",
        "    \"            action_tanh = torch.clamp(action_tanh, -0.9999, 0.9999)\\n\",\n",
        "    \"            # Reverse the Tanh to get the unbounded action\\n\",\n",
        "    \"            action_unbounded = torch.atanh(action_tanh)\\n\",\n",
        "    \"            \\n\",\n",
        "    \"        # Log-prob of the scaled action\\n\",\n",
        "    \"        log_prob = probs.log_prob(action_unbounded)\\n\",\n",
        "    \"        log_prob -= torch.log(self.action_scale * (1 - action_tanh.pow(2)) + 1e-6)\\n\",\n",
        "    \"        log_prob = log_prob.sum(1, keepdim=True)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        entropy = probs.entropy().sum(1)\\n\",\n",
        "    \"        value = self.critic(x)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        return action, log_prob, entropy, value\\n\",\n",
        "    \"\\n\",\n",
        "    \"def main():\\n\",\n",
        "    \"    # Set device\\n\",\n",
        "    \"    device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\",\n",
        "    \"    print(f\\\"Using device: {device}\\\")\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    print(\\\"Initializing Go2 RL Environment...\\\")\\n\",\n",
        "    \"    env = Go2Env()\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    obs_dim = env.observation_space.shape[0]\\n\",\n",
        "    \"    action_dim = env.action_space.shape[0]\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    print(f\\\"Observation space dim: {obs_dim}\\\")\\n\",\n",
        "    \"    print(f\\\"Action space dim: {action_dim}\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"    # --- PPO Agent ---\\n\",\n",
        "    \"    agent = ActorCritic(obs_dim, action_dim, env.action_low, env.action_high).to(device)\\n\",\n",
        "    \"    optimizer = optim.Adam(agent.parameters(), lr=config.PPO_LEARNING_RATE, eps=1e-5)\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    # --- Storage ---\\n\",\n",
        "    \"    num_steps = config.PPO_STEPS_PER_EPOCH\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    obs = torch.zeros((num_steps, obs_dim)).to(device)\\n\",\n",
        "    \"    actions = torch.zeros((num_steps, action_dim)).to(device)\\n\",\n",
        "    \"    logprobs = torch.zeros(num_steps).to(device)\\n\",\n",
        "    \"    rewards = torch.zeros(num_steps).to(device)\\n\",\n",
        "    \"    dones = torch.zeros(num_steps).to(device)\\n\",\n",
        "    \"    values = torch.zeros(num_steps).to(device)\\n\",\n",
        "    \"\\n\",\n",
        "    \"    print(\\\"Starting PPO Training...\\\")\\n\",\n",
        "    \"    start_time = time.time()\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    next_obs, _ = env.reset()\\n\",\n",
        "    \"    next_obs = torch.Tensor(next_obs).to(device)\\n\",\n",
        "    \"    next_done = torch.zeros(1).to(device)\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    for epoch in range(config.PPO_EPOCHS):\\n\",\n",
        "    \"        epoch_rewards = []\\n\",\n",
        "    \"        epoch_reward_components = {}\\n\",\n",
        "    \"\\n\",\n",
        "    \"        for step in range(num_steps):\\n\",\n",
        "    \"            obs[step] = next_obs\\n\",\n",
        "    \"            dones[step] = next_done\\n\",\n",
        "    \"\\n\",\n",
        "    \"            # Get action and value from agent\\n\",\n",
        "    \"            with torch.no_grad():\\n\",\n",
        "    \"                action, logprob, _, value = agent.get_action_and_value(next_obs.unsqueeze(0))\\n\",\n",
        "    \"                values[step] = value.flatten()\\n\",\n",
        "    \"            \\n\",\n",
        "    \"            actions[step] = action.squeeze(0)\\n\",\n",
        "    \"            logprobs[step] = logprob.squeeze()\\n\",\n",
        "    \"\\n\",\n",
        "    \"            # Step the environment\\n\",\n",
        "    \"            next_obs_np, reward, terminated, truncated, info = env.step(action.cpu().numpy().squeeze(0))\\n\",\n",
        "    \"            epoch_rewards.append(reward)\\n\",\n",
        "    \"            \\n\",\n",
        "    \"            # Log reward components\\n\",\n",
        "    \"            for key, val in info.items():\\n\",\n",
        "    \"                if key not in epoch_reward_components:\\n\",\n",
        "    \"                    epoch_reward_components[key] = []\\n\",\n",
        "    \"                epoch_reward_components[key].append(val)\\n\",\n",
        "    \"            \\n\",\n",
        "    \"            rewards[step] = torch.tensor(reward, device=device).view(-1)\\n\",\n",
        "    \"            next_obs = torch.Tensor(next_obs_np).to(device)\\n\",\n",
        "    \"            next_done = torch.tensor(float(terminated or truncated), device=device)\\n\",\n",
        "    \"            \\n\",\n",
        "    \"            if next_done:\\n\",\n",
        "    \"                epoch_rewards = []\\n\",\n",
        "    \"                epoch_reward_components = {}\\n\",\n",
        "    \"                next_obs, _ = env.reset()\\n\",\n",
        "    \"                next_obs = torch.Tensor(next_obs).to(device)\\n\",\n",
        "    \"\\n\",\n",
        "    \"        # --- Calculate Advantages (GAE) ---\\n\",\n",
        "    \"        with torch.no_grad():\\n\",\n",
        "    \"            next_value = agent.get_value(next_obs.unsqueeze(0)).reshape(1, -1)\\n\",\n",
        "    \"            advantages = torch.zeros_like(rewards).to(device)\\n\",\n",
        "    \"            lastgaelam = 0\\n\",\n",
        "    \"            for t in reversed(range(num_steps)):\\n\",\n",
        "    \"                if t == num_steps - 1:\\n\",\n",
        "    \"                    nextnonterminal = 1.0 - next_done\\n\",\n",
        "    \"                    nextvalues = next_value\\n\",\n",
        "    \"                else:\\n\",\n",
        "    \"                    nextnonterminal = 1.0 - dones[t + 1]\\n\",\n",
        "    \"                    nextvalues = values[t + 1]\\n\",\n",
        "    \"                \\n\",\n",
        "    \"                delta = rewards[t] + config.PPO_GAMMA * nextvalues * nextnonterminal - values[t]\\n\",\n",
        "    \"                advantages[t] = lastgaelam = delta + config.PPO_GAMMA * config.PPO_LAM * nextnonterminal * lastgaelam\\n\",\n",
        "    \"            returns = advantages + values\\n\",\n",
        "    \"\\n\",\n",
        "    \"        # --- Update Policy ---\\n\",\n",
        "    \"        b_obs = obs.reshape((-1,) + env.observation_space.shape)\\n\",\n",
        "    \"        b_actions = actions.reshape((-1,) + env.action_space.shape)\\n\",\n",
        "    \"        b_logprobs = logprobs.reshape(-1)\\n\",\n",
        "    \"        b_advantages = advantages.reshape(-1)\\n\",\n",
        "    \"        b_returns = returns.reshape(-1)\\n\",\n",
        "    \"\\n\",\n",
        "    \"        # Normalize advantages\\n\",\n",
        "    \"        b_advantages = (b_advantages - b_advantages.mean()) / (b_advantages.std() + 1e-8)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        b_inds = np.arange(num_steps)\\n\",\n",
        "    \"        for _ in range(config.PPO_UPDATE_EPOCHS):\\n\",\n",
        "    \"            np.random.shuffle(b_inds)\\n\",\n",
        "    \"            for start in range(0, num_steps, config.PPO_MINIBATCH_SIZE):\\n\",\n",
        "    \"                end = start + config.PPO_MINIBATCH_SIZE\\n\",\n",
        "    \"                mb_inds = b_inds[start:end]\\n\",\n",
        "    \"\\n\",\n",
        "    \"                _, newlogprob, entropy, newvalue = agent.get_action_and_value(\\n\",\n",
        "    \"                    b_obs[mb_inds], b_actions[mb_inds]\\n\",\n",
        "    \"                )\\n\",\n",
        "    \"                logratio = newlogprob.squeeze() - b_logprobs[mb_inds]\\n\",\n",
        "    \"                ratio = logratio.exp()\\n\",\n",
        "    \"\\n\",\n",
        "    \"                # Policy loss\\n\",\n",
        "    \"                pg_loss1 = -b_advantages[mb_inds] * ratio\\n\",\n",
        "    \"                pg_loss2 = -b_advantages[mb_inds] * torch.clamp(\\n\",\n",
        "    \"                    ratio, 1 - config.PPO_CLIP, 1 + config.PPO_CLIP\\n\",\n",
        "    \"                )\\n\",\n",
        "    \"                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\\n\",\n",
        "    \"\\n\",\n",
        "    \"                # Value loss\\n\",\n",
        "    \"                v_loss = 0.5 * ((newvalue.squeeze() - b_returns[mb_inds]) ** 2).mean()\\n\",\n",
        "    \"\\n\",\n",
        "    \"                # Entropy loss\\n\",\n",
        "    \"                entropy_loss = entropy.mean()\\n\",\n",
        "    \"\\n\",\n",
        "    \"                # Total loss\\n\",\n",
        "    \"                loss = pg_loss - 0.01 * entropy_loss + v_loss * 0.5\\n\",\n",
        "    \"\\n\",\n",
        "    \"                optimizer.zero_grad()\\n\",\n",
        "    \"                loss.backward()\\n\",\n",
        "    \"                nn.utils.clip_grad_norm_(agent.parameters(), 0.5)\\n\",\n",
        "    \"                optimizer.step()\\n\",\n",
        "    \"\\n\",\n",
        "    \"        # --- Logging ---\\n\",\n",
        "    \"        num_episodes = dones.sum().item()\\n\",\n",
        "    \"        if num_episodes == 0:\\n\",\n",
        "    \"            avg_reward = np.nan # Avoid division by zero if no episodes finished\\n\",\n",
        "    \"        else:\\n\",\n",
        "    \"            avg_reward = rewards.sum().item() / num_episodes\\n\",\n",
        "    \"            \\n\",\n",
        "    \"        print(f\\\"Epoch {epoch+1}/{config.PPO_EPOCHS} | Avg. Ep Reward: {avg_reward:.2f} | Time: {time.time()-start_time:.2f}s\\\")\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        # Log mean of reward components\\n\",\n",
        "    \"        for key, val_list in epoch_reward_components.items():\\n\",\n",
        "    \"            if val_list:\\n\",\n",
        "    \"                print(f\\\"  ... avg {key}: {np.mean(val_list):.3f}\\\")\\n\",\n",
        "    \"        \\n\",\n",
        "    \"    env.close()\\n\",\n",
        "    \"    print(\\\"Training finished.\\\")\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    # Save the trained policy\\n\",\n",
        "    \"    model_path = \\\"ppo_go2_policy.pth\\\"\\n\",\n",
        "    \"    torch.save(agent.state_dict(), model_path)\\n\",\n",
        "    \"    print(f\\\"Trained policy saved to {model_path}\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"if __name__ == \\\"__main__\\\":\\n\",\n",
        "    \"    main()\\n\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"## Step 4: Run Training\\n\",\n",
        "    \"\\n\",\n",
        "    \"This final cell executes the training script. It will print the average reward for each epoch. Training will take a while!\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"!python train_ppo.py\"\n",
        "   ]\n",
        "  }\n",
        " ],\n",
        " \"metadata\": {\n",
        "  \"kernelspec\": {\n",
        "   \"display_name\": \"Python 3\",\n",
        "   \"language\": \"python\",\n",
        "   \"name\": \"python3\"\n",
        "  },\n",
        "  \"language_info\": {\n",
        "   \"codemirror_mode\": {\n",
        "    \"name\": \"ipython\",\n",
        "    \"version\": 3\n",
        "   },\n",
        "   \"file_extension\": \".py\",\n",
        "   \"mimetype\": \"text/x-python\",\n",
        "   \"name\": \"python\",\n",
        "   \"nbconvert_exporter\": \"python\",\n",
        "   \"pygments_lexer\": \"ipython3\",\n",
        "   \"version\": \"3.10.12\"\n",
        "  }\n",
        " },\n",
        " \"nbformat\": 4,\n",
        " \"nbformat_minor\": 4\n",
        "}\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "8OebChPGA7_F",
        "outputId": "7830671d-136f-42d0-f27f-89e607b3d886"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' # @title\\n{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"# Go2 Quadruped RL Trainer (MuJoCo + PPO)\\n\",\\n    \"\\n\",\\n    \"This notebook will set up a complete environment for training the Unitree Go2 robot to walk using Reinforcement Learning (PPO). \\n\",\\n    \"\\n\",\\n    \"**Instructions:**\\n\",\\n    \"1.  Make sure your runtime is set to use a GPU (Runtime > Change runtime type > T4 GPU) for faster training.\\n\",\\n    \"2.  Run the cells in order from top to bottom.\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"## Step 1: Install Dependencies\\n\",\\n    \"\\n\",\\n    \"This cell installs all necessary Python libraries for the simulation and RL algorithm.\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"!pip install -q mujoco gymnasium torch scipy\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"## Step 2: Get Robot Model and Assets\\n\",\\n    \"\\n\",\\n    \"We clone the official `unitree_mujoco` repository. This gives us the `go2.xml` file and, most importantly, the `assets/` folder containing all the `.obj` mesh files the robot model needs to load.\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"!git clone https://github.com/unitreerobotics/unitree_mujoco\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"## Step 3: Create Python Training Files\\n\",\\n    \"\\n\",\\n    \"We use the `%%writefile` magic command to create our four Python scripts in the Colab environment\\'s main directory.\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"%%writefile config.py\\n\",\\n    \"import numpy as np\\n\",\\n    \"\\n\",\\n    \"# --- Simulation ---\\n\",\\n    \"# This path now points inside the cloned repository\\n\",\\n    \"XML_PATH = \\'unitree_mujoco/unitree_robots/go2/go2.xml\\'\\n\",\\n    \"\\n\",\\n    \"SIM_HZ = 500  # (Hz) Frequency of MuJoCo physics steps\\n\",\\n    \"CONTROL_HZ = 50  # (Hz) Frequency of RL agent policy decisions\\n\",\\n    \"FRAME_SKIP = SIM_HZ // CONTROL_HZ # Number of physics steps per agent step\\n\",\\n    \"MAX_EPISODE_STEPS = 1000  # Max steps before env reset\\n\",\\n    \"\\n\",\\n    \"# --- Robot Model ---\\n\",\\n    \"# These names are taken from the official \\'go2.xml\\'\\n\",\\n    \"TRUNK_BODY_NAME = \"trunk\"\\n\",\\n    \"JOINT_NAMES = [\\n\",\\n    \"    \"FR_hip_joint\", \"FR_thigh_joint\", \"FR_calf_joint\",\\n\",\\n    \"    \"FL_hip_joint\", \"FL_thigh_joint\", \"FL_calf_joint\",\\n\",\\n    \"    \"RR_hip_joint\", \"RR_thigh_joint\", \"RR_calf_joint\",\\n\",\\n    \"    \"RL_hip_joint\", \"RL_thigh_joint\", \"RL_calf_joint\",\\n\",\\n    \"]\\n\",\\n    \"# We use the calf bodies to check for contact, as they contain the foot geoms\\n\",\\n    \"FOOT_BODY_NAMES = [\"FR_calf\", \"FL_calf\", \"RR_calf\", \"RL_calf\"]\\n\",\\n    \"\\n\",\\n    \"# --- Locomotion ---\\n\",\\n    \"TARGET_VELOCITY = 0.8  # (m/s) Target forward velocity (x-axis)\\n\",\\n    \"TARGET_HEIGHT = 0.3    # (m) Target CoM height\\n\",\\n    \"CONTACT_FORCE_THRESHOLD = 5.0  # (N) Force threshold to register foot contact\\n\",\\n    \"\\n\",\\n    \"# --- RL Reward Weights ---\\n\",\\n    \"W_VEL_X = 2.0         # Reward for matching target x-velocity\\n\",\\n    \"W_VEL_Y = -1.0        # Penalty for y-velocity\\n\",\\n    \"W_VEL_Z = -1.0        # Penalty for z-velocity\\n\",\\n    \"W_ANG_VEL = -0.1      # Penalty for angular velocity\\n\",\\n    \"W_COM_HEIGHT = 1.5    # Reward for maintaining target height\\n\",\\n    \"W_ORIENTATION = -2.0  # Penalty for roll and pitch\\n\",\\n    \"W_ACTION_RATE = -0.01 # Penalty for jerky actions\\n\",\\n    \"W_TORQUE = -0.00002   # Penalty for motor effort (torques)\\n\",\\n    \"W_CONTACT_FORCE = -0.0001 # Penalty for high contact forces\\n\",\\n    \"W_COM_IN_SUPPORT = 3.0 # Reward for keeping CoM in support polygon\\n\",\\n    \"W_FALL = -200.0       # Large penalty for falling\\n\",\\n    \"\\n\",\\n    \"# --- PPO Training ---\\n\",\\n    \"PPO_STEPS_PER_EPOCH = 4096\\n\",\\n    \"PPO_EPOCHS = 500\\n\",\\n    \"PPO_LEARNING_RATE = 3e-4\\n\",\\n    \"PPO_MINIBATCH_SIZE = 64\\n\",\\n    \"PPO_UPDATE_EPOCHS = 10\\n\",\\n    \"PPO_GAMMA = 0.99\\n\",\\n    \"PPO_LAM = 0.95\\n\",\\n    \"PPO_CLIP = 0.2\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"%%writefile utils.py\\n\",\\n    \"import numpy as np\\n\",\\n    \"import mujoco\\n\",\\n    \"from scipy.spatial import ConvexHull, Delaunay\\n\",\\n    \"\\n\",\\n    \"# --- MuJoCo Model/Data Getters ---\\n\",\\n    \"\\n\",\\n    \"def get_body_id(model, body_name):\\n\",\\n    \"    \"\"\"Returns the MuJoCo ID for a body.\"\"\"\\n\",\\n    \"    return mujoco.mj_name2id(model, mujoco.mjtObj.mjOBJ_BODY, body_name)\\n\",\\n    \"\\n\",\\n    \"def get_joint_qpos_ids(model, joint_names):\\n\",\\n    \"    \"\"\"Returns qpos indices for a list of joint names.\"\"\"\\n\",\\n    \"    return [model.jnt_qposadr[mujoco.mj_name2id(model, mujoco.mjtObj.mjOBJ_JOINT, name)] for name in joint_names]\\n\",\\n    \"\\n\",\\n    \"def get_joint_qvel_ids(model, joint_names):\\n\",\\n    \"    \"\"\"Returns qvel indices for a list of joint names.\"\"\"\\n\",\\n    \"    return [model.jnt_dofadr[mujoco.mj_name2id(model, mujoco.mjtObj.mjOBJ_JOINT, name)] for name in joint_names]\\n\",\\n    \"\\n\",\\n    \"def get_actuator_ids(model, joint_names):\\n\",\\n    \"    \"\"\"Returns actuator indices for a list of joint names.\"\"\"\\n\",\\n    \"    # Assumes motor name is joint name + \"_motor\"\\n\",\\n    \"    return [mujoco.mj_name2id(model, mujoco.mjtObj.mjOBJ_ACTUATOR, name.replace(\"_joint\", \"_motor\")) for name in joint_names]\\n\",\\n    \"\\n\",\\n    \"def get_foot_body_ids(model, foot_body_names):\\n\",\\n    \"    \"\"\"Returns body IDs for a list of foot body names.\"\"\"\\n\",\\n    \"    return [get_body_id(model, name) for name in foot_body_names]\\n\",\\n    \"\\n\",\\n    \"# --- Kinematics & Dynamics ---\\n\",\\n    \"\\n\",\\n    \"def get_com_position(data, trunk_id):\\n\",\\n    \"    \"\"\"Returns the 3D position of the trunk (CoM).\"\"\"\\n\",\\n    \"    return data.xpos[trunk_id]\\n\",\\n    \"\\n\",\\n    \"def get_com_velocity(data, trunk_id):\\n\",\\n    \"    \"\"\"Returns the 3D linear velocity of the trunk (CoM).\"\"\"\\n\",\\n    \"    # Use cvel (velocities in world frame)\\n\",\\n    \"    return data.cvel[trunk_id, 3:6]\\n\",\\n    \"\\n\",\\n    \"def get_body_orientation(data, trunk_id):\\n\",\\n    \"    \"\"\"Returns the quaternion orientation of the trunk.\"\"\"\\n\",\\n    \"    return data.xquat[trunk_id]\\n\",\\n    \"\\n\",\\n    \"def get_body_angular_velocity(data, trunk_id):\\n\",\\n    \"    \"\"\"Returns the 3D angular velocity of the trunk.\"\"\"\\n\",\\n    \"    # Use cvel (velocities in world frame)\\n\",\\n    \"    return data.cvel[trunk_id, 0:3]\\n\",\\n    \"\\n\",\\n    \"def quat_to_rpy(quat):\\n\",\\n    \"    \"\"\"Converts a quaternion (w, x, y, z) to roll, pitch, yaw.\"\"\"\\n\",\\n    \"    w, x, y, z = quat\\n\",\\n    \"    \\n\",\\n    \"    # Roll (x-axis rotation)\\n\",\\n    \"    sinr_cosp = 2 * (w * x + y * z)\\n\",\\n    \"    cosr_cosp = 1 - 2 * (x * x + y * y)\\n\",\\n    \"    roll = np.arctan2(sinr_cosp, cosr_cosp)\\n\",\\n    \"    \\n\",\\n    \"    # Pitch (y-axis rotation)\\n\",\\n    \"    sinp = 2 * (w * y - z * x)\\n\",\\n    \"    if np.abs(sinp) >= 1:\\n\",\\n    \"        pitch = np.copysign(np.pi / 2, sinp)  # Use 90 degrees if out of range\\n\",\\n    \"    else:\\n\",\\n    \"        pitch = np.arcsin(sinp)\\n\",\\n    \"        \\n\",\\n    \"    # Yaw (z-axis rotation)\\n\",\\n    \"    siny_cosp = 2 * (w * z + x * y)\\n\",\\n    \"    cosy_cosp = 1 - 2 * (y * y + z * z)\\n\",\\n    \"    yaw = np.arctan2(siny_cosp, cosy_cosp)\\n\",\\n    \"    \\n\",\\n    \"    return roll, pitch, yaw\\n\",\\n    \"\\n\",\\n    \"# --- Contact & Stability ---\\n\",\\n    \"\\n\",\\n    \"def get_foot_contacts(model, data, foot_body_ids, contact_force_threshold):\\n\",\\n    \"    \"\"\"\\n\",\\n    \"    Checks for foot contact with the ground.\\n\",\\n    \"    Returns a boolean array [FR, FL, RR, RL]\\n\",\\n    \"    \"\"\"\\n\",\\n    \"    contacts = [False] * 4\\n\",\\n    \"    for i in range(data.ncon):\\n\",\\n    \"        contact = data.contact[i]\\n\",\\n    \"        \\n\",\\n    \"        # Check if geom1 or geom2 is a foot\\n\",\\n    \"        geom1_body = model.geom_bodyid[contact.geom1]\\n\",\\n    \"        geom2_body = model.geom_bodyid[contact.geom2]\\n\",\\n    \"        \\n\",\\n    \"        is_geom1_foot = geom1_body in foot_body_ids\\n\",\\n    \"        is_geom2_foot = geom2_body in foot_body_ids\\n\",\\n    \"        \\n\",\\n    \"        if not (is_geom1_foot or is_geom2_foot):\\n\",\\n    \"            continue # Not a foot contact\\n\",\\n    \"\\n\",\\n    \"        # Check if the other geom is the ground (geom ID 0)\\n\",\\n    \"        is_geom1_ground = contact.geom1 == 0\\n\",\\n    \"        is_geom2_ground = contact.geom2 == 0\\n\",\\n    \"\\n\",\\n    \"        if not (is_geom1_ground or is_geom2_ground):\\n\",\\n    \"            continue # Not a ground contact\\n\",\\n    \"            \\n\",\\n    \"        # Get contact force\\n\",\\n    \"        force_normal = np.zeros(3)\\n\",\\n    \"        mujoco.mj_contactForce(model, data, i, force_normal)\\n\",\\n    \"        \\n\",\\n    \"        if np.linalg.norm(force_normal) > contact_force_threshold:\\n\",\\n    \"            if is_geom1_foot:\\n\",\\n    \"                foot_idx = foot_body_ids.index(geom1_body)\\n\",\\n    \"                contacts[foot_idx] = True\\n\",\\n    \"            if is_geom2_foot:\\n\",\\n    \"                foot_idx = foot_body_ids.index(geom2_body)\\n\",\\n    \"                contacts[foot_idx] = True\\n\",\\n    \"                \\n\",\\n    \"    return np.array(contacts)\\n\",\\n    \"\\n\",\\n    \"def get_foot_positions(data, foot_body_ids):\\n\",\\n    \"    \"\"\"Returns the 3D world positions of the feet (calf bodies).\"\"\"\\n\",\\n    \"    return data.xpos[foot_body_ids]\\n\",\\n    \"\\n\",\\n    \"def get_support_polygon(foot_positions, contact_states):\\n\",\\n    \"    \"\"\"\\n\",\\n    \"    Returns the 2D vertices (x, y) of the support polygon.\\n\",\\n    \"    Returns an empty list if fewer than 2 feet are in contact.\\n\",\\n    \"    \"\"\"\\n\",\\n    \"    stance_feet_pos = foot_positions[contact_states, :2] # Get (x, y) of stance feet\\n\",\\n    \"    \\n\",\\n    \"    if stance_feet_pos.shape[0] < 2:\\n\",\\n    \"        return [] # Not enough points to form a polygon\\n\",\\n    \"        \\n\",\\n    \"    if stance_feet_pos.shape[0] == 2:\\n\",\\n    \"        return stance_feet_pos # Support polygon is a line\\n\",\\n    \"        \\n\",\\n    \"    try:\\n\",\\n    \"        # A Bounding Box is simpler and more stable than Convex Hull for 3 points\\n\",\\n    \"        if stance_feet_pos.shape[0] == 3:\\n\",\\n    \"             return stance_feet_pos\\n\",\\n    \"        \\n\",\\n    \"        hull = ConvexHull(stance_feet_pos)\\n\",\\n    \"        return stance_feet_pos[hull.vertices]\\n\",\\n    \"    except Exception:\\n\",\\n    \"        return [] # Error during hull calculation (e.g., colinear points)\\n\",\\n    \"\\n\",\\n    \"def is_com_stable(com_pos_2d, support_polygon):\\n\",\\n    \"    \"\"\"\\n\",\\n    \"    Checks if the 2D CoM position is inside the 2D support polygon.\\n\",\\n    \"    Uses scipy.spatial.Delaunay for robust point-in-polygon check.\\n\",\\n    \"    \"\"\"\\n\",\\n    \"    if len(support_polygon) < 3:\\n\",\\n    \"        # If support is a line (2 feet) or point (1 foot),\\n\",\\n    \"        # we can\\'t use a polygon check.\\n\",\\n    \"        # For simplicity, we\\'ll call it \"unstable\"\\n\",\\n    \"        return False\\n\",\\n    \"        \\n\",\\n    \"    try:\\n\",\\n    \"        # Create a Delaunay triangulation of the support polygon\\n\",\\n    \"        hull = Delaunay(support_polygon)\\n\",\\n    \"        \\n\",\\n    \"        # find_simplex returns -1 if the point is outside the hull\\n\",\\n    \"        return hull.find_simplex(com_pos_2d) >= 0\\n\",\\n    \"    except Exception:\\n\",\\n    \"        # Error (e.g., flat polygon)\\n\",\\n    \"        return False\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"%%writefile go2_env.py\\n\",\\n    \"import mujoco\\n\",\\n    \"import gymnasium as gym\\n\",\\n    \"from gymnasium import spaces\\n\",\\n    \"import numpy as np\\n\",\\n    \"import os\\n\",\\n    \"\\n\",\\n    \"import config\\n\",\\n    \"import utils\\n\",\\n    \"\\n\",\\n    \"class Go2Env(gym.Env):\\n\",\\n    \"    \"\"\"\\n\",\\n    \"    Custom Gymnasium environment for the Unitree Go2 robot using MuJoCo.\\n\",\\n    \"    \"\"\"\\n\",\\n    \"    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 50}\\n\",\\n    \"\\n\",\\n    \"    def __init__(self, render_mode=None):\\n\",\\n    \"        super().__init__()\\n\",\\n    \"        \\n\",\\n    \"        self.render_mode = render_mode\\n\",\\n    \"        \\n\",\\n    \"        # Construct the full path to the XML file\\n\",\\n    \"        # This path is relative to where the script is run (e.g., /content/)\\n\",\\n    \"        xml_path = config.XML_PATH\\n\",\\n    \"        \\n\",\\n    \"        if not os.path.exists(xml_path):\\n\",\\n    \"            raise FileNotFoundError(\\n\",\\n    \"                f\"Could not find XML file: {xml_path}. \"\\n\",\\n    \"                f\"Make sure the \\'unitree_mujoco\\' repo was cloned successfully.\"\\n\",\\n    \"            )\\n\",\\n    \"            \\n\",\\n    \"        self.model = mujoco.MjModel.from_xml_path(xml_path)\\n\",\\n    \"        self.data = mujoco.MjData(self.model)\\n\",\\n    \"        \\n\",\\n    \"        # --- Get element IDs from model ---\\n\",\\n    \"        self.trunk_id = utils.get_body_id(self.model, config.TRUNK_BODY_NAME)\\n\",\\n    \"        self.joint_qpos_ids = utils.get_joint_qpos_ids(self.model, config.JOINT_NAMES)\\n\",\\n    \"        self.joint_qvel_ids = utils.get_joint_qvel_ids(self.model, config.JOINT_NAMES)\\n\",\\n    \"        self.actuator_ids = utils.get_actuator_ids(self.model, config.JOINT_NAMES)\\n\",\\n    \"        self.foot_body_ids = utils.get_foot_body_ids(self.model, config.FOOT_BODY_NAMES)\\n\",\\n    \"        \\n\",\\n    \"        # Store initial state for resets\\n\",\\n    \"        self.init_qpos = self.data.qpos.copy()\\n\",\\n    \"        self.init_qvel = self.data.qvel.copy()\\n\",\\n    \"        self.action_history = np.zeros(12)\\n\",\\n    \"        \\n\",\\n    \"        # --- Define Action Space ---\\n\",\\n    \"        act_dim = 12\\n\",\\n    \"        ctrl_range = self.model.actuator_ctrlrange[self.actuator_ids]\\n\",\\n    \"        self.action_low = ctrl_range[:, 0]\\n\",\\n    \"        self.action_high = ctrl_range[:, 1]\\n\",\\n    \"        \\n\",\\n    \"        self.action_space = spaces.Box(\\n\",\\n    \"            low=self.action_low,\\n\",\\n    \"            high=self.action_high,\\n\",\\n    \"            dtype=np.float32\\n\",\\n    \"        )\\n\",\\n    \"        \\n\",\\n    \"        # --- Define Observation Space ---\\n\",\\n    \"        obs_dim = 3 + 3 + 4 + 12 + 12 + 4 # com_vel, ang_vel, quat, qpos, qvel, contacts\\n\",\\n    \"        self.observation_space = spaces.Box(\\n\",\\n    \"            low=-np.inf,\\n\",\\n    \"            high=np.inf,\\n\",\\n    \"            shape=(obs_dim,),\\n\",\\n    \"            dtype=np.float64\\n\",\\n    \"        )\\n\",\\n    \"        \\n\",\\n    \"        # --- Rendering (Colab) ---\\n\",\\n    \"        # We\\'ll use \\'rgb_array\\' for Colab. \\'human\\' mode won\\'t work.\\n\",\\n    \"        if self.render_mode == \"human\":\\n\",\\n    \"            print(\"Warning: \\'human\\' render_mode not supported in Colab. Use \\'rgb_array\\' instead.\")\\n\",\\n    \"            self.render_mode = \"rgb_array\"\\n\",\\n    \"            \\n\",\\n    \"        if self.render_mode == \"rgb_array\":\\n\",\\n    \"            self.renderer = mujoco.Renderer(self.model, 480, 640)\\n\",\\n    \"        else:\\n\",\\n    \"            self.renderer = None\\n\",\\n    \"\\n\",\\n    \"    def _get_obs(self):\\n\",\\n    \"        \"\"\"Constructs the observation vector from simulation data.\"\"\"\\n\",\\n    \"        com_vel = utils.get_com_velocity(self.data, self.trunk_id)\\n\",\\n    \"        ang_vel = utils.get_body_angular_velocity(self.data, self.trunk_id)\\n\",\\n    \"        quat = utils.get_body_orientation(self.data, self.trunk_id)\\n\",\\n    \"        qpos = self.data.qpos[self.joint_qpos_ids]\\n\",\\n    \"        qvel = self.data.qvel[self.joint_qvel_ids]\\n\",\\n    \"        contacts = utils.get_foot_contacts(\\n\",\\n    \"            self.model, self.data, self.foot_body_ids, config.CONTACT_FORCE_THRESHOLD\\n\",\\n    \"        )\\n\",\\n    \"        \\n\",\\n    \"        return np.concatenate([\\n\",\\n    \"            com_vel, ang_vel, quat, qpos, qvel, contacts.astype(float)\\n\",\\n    \"        ])\\n\",\\n    \"\\n\",\\n    \"    def _compute_reward(self, action):\\n\",\\n    \"        \"\"\"Calculates the reward based on the current state and action.\"\"\"\\n\",\\n    \"        \\n\",\\n    \"        # --- Get current state data ---\\n\",\\n    \"        com_pos = utils.get_com_position(self.data, self.trunk_id)\\n\",\\n    \"        com_vel = utils.get_com_velocity(self.data, self.trunk_id)\\n\",\\n    \"        ang_vel = utils.get_body_angular_velocity(self.data, self.trunk_id)\\n\",\\n    \"        quat = utils.get_body_orientation(self.data, self.trunk_id)\\n\",\\n    \"        torques = self.data.actuator_force[self.actuator_ids]\\n\",\\n    \"        \\n\",\\n    \"        # 1. Velocity Tracking (X, Y, Z)\\n\",\\n    \"        vel_error_x = (com_vel[0] - config.TARGET_VELOCITY)**2\\n\",\\n    \"        vel_error_y = com_vel[1]**2\\n\",\\n    \"        vel_error_z = com_vel[2]**2\\n\",\\n    \"        \\n\",\\n    \"        reward_vel_x = config.W_VEL_X * np.exp(-vel_error_x * 5.0)\\n\",\\n    \"        penalty_vel_y = config.W_VEL_Y * vel_error_y\\n\",\\n    \"        penalty_vel_z = config.W_VEL_Z * vel_error_z\\n\",\\n    \"        \\n\",\\n    \"        # 2. CoM Height\\n\",\\n    \"        height_error = (com_pos[2] - config.TARGET_HEIGHT)**2\\n\",\\n    \"        reward_height = config.W_COM_HEIGHT * np.exp(-height_error * 20.0)\\n\",\\n    \"        \\n\",\\n    \"        # 3. Orientation\\n\",\\n    \"        roll, pitch, _ = utils.quat_to_rpy(quat)\\n\",\\n    \"        orientation_penalty = config.W_ORIENTATION * (roll**2 + pitch**2)\\n\",\\n    \"        ang_vel_penalty = config.W_ANG_VEL * np.sum(ang_vel**2)\\n\",\\n    \"        \\n\",\\n    \"        # 4. Effort / Torque / Action Rate\\n\",\\n    \"        torque_penalty = config.W_TORQUE * np.sum(torques**2)\\n\",\\n    \"        action_rate_penalty = config.W_ACTION_RATE * np.sum((action - self.action_history)**2)\\n\",\\n    \"        self.action_history = action # store for next step\\n\",\\n    \"\\n\",\\n    \"        # 5. Stability: CoM within Support Polygon\\n\",\\n    \"        contacts = utils.get_foot_contacts(\\n\",\\n    \"            self.model, self.data, self.foot_body_ids, config.CONTACT_FORCE_THRESHOLD\\n\",\\n    \"        )\\n\",\\n    \"        foot_positions = utils.get_foot_positions(self.data, self.foot_body_ids)\\n\",\\n    \"        support_polygon = utils.get_support_polygon(foot_positions, contacts)\\n\",\\n    \"        \\n\",\\n    \"        is_stable = utils.is_com_stable(com_pos[:2], support_polygon)\\n\",\\n    \"        reward_com_stable = config.W_COM_IN_SUPPORT if is_stable else 0.0\\n\",\\n    \"\\n\",\\n    \"        # --- Sum Rewards ---\\n\",\\n    \"        total_reward = (\\n\",\\n    \"            reward_vel_x + penalty_vel_y + penalty_vel_z +\\n\",\\n    \"            reward_height + \\n\",\\n    \"            orientation_penalty + ang_vel_penalty +\\n\",\\n    \"            torque_penalty + action_rate_penalty +\\n\",\\n    \"            reward_com_stable\\n\",\\n    \"        )\\n\",\\n    \"        \\n\",\\n    \"        reward_info = {\\n\",\\n    \"            \"r_vel_x\": reward_vel_x,\\n\",\\n    \"            \"p_vel_y\": penalty_vel_y,\\n\",\\n    \"            \"p_vel_z\": penalty_vel_z,\\n\",\\n    \"            \"r_height\": reward_height,\\n\",\\n    \"            \"p_orientation\": orientation_penalty,\\n\",\\n    \"            \"p_ang_vel\": ang_vel_penalty,\\n\",\\n    \"            \"p_torque\": torque_penalty,\\n\",\\n    \"            \"p_action_rate\": action_rate_penalty,\\n\",\\n    \"            \"r_com_stable\": reward_com_stable,\\n\",\\n    \"        }\\n\",\\n    \"        \\n\",\\n    \"        return total_reward, reward_info\\n\",\\n    \"\\n\",\\n    \"    def _check_termination(self):\\n\",\\n    \"        \"\"\"Checks if the episode should terminate (e.g., robot fell).\"\"\"\\n\",\\n    \"        com_pos = utils.get_com_position(self.data, self.trunk_id)\\n\",\\n    \"        quat = utils.get_body_orientation(self.data, self.trunk_id)\\n\",\\n    \"        roll, pitch, _ = utils.quat_to_rpy(quat)\\n\",\\n    \"        \\n\",\\n    \"        # Fell if CoM is too low or if roll/pitch is too high\\n\",\\n    \"        is_fallen = (com_pos[2] < 0.15) or (abs(roll) > 1.0) or (abs(pitch) > 1.0)\\n\",\\n    \"        \\n\",\\n    \"        return is_fallen\\n\",\\n    \"\\n\",\\n    \"    def step(self, action):\\n\",\\n    \"        \"\"\"Run one timestep of the environment\\'s dynamics.\"\"\"\\n\",\\n    \"        \\n\",\\n    \"        # Clip action to be safe\\n\",\\n    \"        action = np.clip(action, self.action_low, self.action_high)\\n\",\\n    \"        \\n\",\\n    \"        # Set the target joint positions as the control signal\\n\",\\n    \"        self.data.ctrl[self.actuator_ids] = action\\n\",\\n    \"        \\n\",\\n    \"        # Step the simulation forward\\n\",\\n    \"        mujoco.mj_step(self.model, self.data, nstep=config.FRAME_SKIP)\\n\",\\n    \"        \\n\",\\n    \"        # Get new state, reward, and termination status\\n\",\\n    \"        observation = self._get_obs()\\n\",\\n    \"        terminated = self._check_termination()\\n\",\\n    \"        reward, reward_info = self._compute_reward(action)\\n\",\\n    \"        \\n\",\\n    \"        if terminated:\\n\",\\n    \"            reward += config.W_FALL # Add large fall penalty\\n\",\\n    \"\\n\",\\n    \"        self.step_count += 1\\n\",\\n    \"        truncated = self.step_count >= config.MAX_EPISODE_STEPS\\n\",\\n    \"\\n\",\\n    \"        return observation, reward, terminated, truncated, reward_info\\n\",\\n    \"\\n\",\\n    \"    def reset(self, seed=None):\\n\",\\n    \"        \"\"\"Reset the environment to an initial state.\"\"\"\\n\",\\n    \"        super().reset(seed=seed)\\n\",\\n    \"        \\n\",\\n    \"        self.step_count = 0\\n\",\\n    \"        self.action_history = np.zeros(12)\\n\",\\n    \"\\n\",\\n    \"        mujoco.mj_resetData(self.model, self.data)\\n\",\\n    \"        self.data.qpos[:] = self.init_qpos\\n\",\\n    \"        self.data.qvel[:] = self.init_qvel\\n\",\\n    \"        \\n\",\\n    \"        # Add small random noise to initial joint positions\\n\",\\n    \"        self.data.qpos[self.joint_qpos_ids] += self.np_random.uniform(\\n\",\\n    \"            -0.1, 0.1, size=len(self.joint_qpos_ids)\\n\",\\n    \"        )\\n\",\\n    \"        \\n\",\\n    \"        mujoco.mj_forward(self.model, self.data)\\n\",\\n    \"        \\n\",\\n    \"        observation = self._get_obs()\\n\",\\n    \"        return observation, {}\\n\",\\n    \"\\n\",\\n    \"    def render(self):\\n\",\\n    \"        \"\"\"Render the environment (if in \\'rgb_array\\' mode).\"\"\"\\n\",\\n    \"        if self.renderer:\\n\",\\n    \"            self.renderer.update_scene(self.data)\\n\",\\n    \"            return self.renderer.render()\\n\",\\n    \"        return None\\n\",\\n    \"\\n\",\\n    \"    def close(self):\\n\",\\n    \"        \"\"\"Close the viewer.\"\"\"\\n\",\\n    \"        if self.renderer:\\n\",\\n    \"            self.renderer = None\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"%%writefile train_ppo.py\\n\",\\n    \"import torch\\n\",\\n    \"import torch.nn as nn\\n\",\\n    \"import torch.optim as optim\\n\",\\n    \"from torch.distributions.normal import Normal\\n\",\\n    \"import numpy as np\\n\",\\n    \"import time\\n\",\\n    \"import os\\n\",\\n    \"\\n\",\\n    \"from go2_env import Go2Env  # Import the custom environment\\n\",\\n    \"import config  # Import config file\\n\",\\n    \"\\n\",\\n    \"class ActorCritic(nn.Module):\\n\",\\n    \"    \"\"\"PPO Actor-Critic network.\"\"\"\\n\",\\n    \"    def __init__(self, obs_dim, action_dim, action_low, action_high):\\n\",\\n    \"        super().__init__()\\n\",\\n    \"        \\n\",\\n    \"        # Store action scaling parameters\\n\",\\n    \"        self.action_low = torch.tensor(action_low, dtype=torch.float32)\\n\",\\n    \"        self.action_high = torch.tensor(action_high, dtype=torch.float32)\\n\",\\n    \"        self.action_scale = (self.action_high - self.action_low) / 2.0\\n\",\\n    \"        self.action_bias = (self.action_high + self.action_low) / 2.0\\n\",\\n    \"\\n\",\\n    \"        hidden_dim = 256\\n\",\\n    \"        \\n\",\\n    \"        # Critic network\\n\",\\n    \"        self.critic = nn.Sequential(\\n\",\\n    \"            nn.Linear(obs_dim, hidden_dim),\\n\",\\n    \"            nn.ReLU(),\\n\",\\n    \"            nn.Linear(hidden_dim, hidden_dim),\\n\",\\n    \"            nn.ReLU(),\\n\",\\n    \"            nn.Linear(hidden_dim, 1)\\n\",\\n    \"        )\\n\",\\n    \"        \\n\",\\n    \"        # Actor network\\n\",\\n    \"        self.actor = nn.Sequential(\\n\",\\n    \"            nn.Linear(obs_dim, hidden_dim),\\n\",\\n    \"            nn.ReLU(),\\n\",\\n    \"            nn.Linear(hidden_dim, hidden_dim),\\n\",\\n    \"            nn.ReLU(),\\n\",\\n    \"            nn.Linear(hidden_dim, action_dim)\\n\",\\n    \"        )\\n\",\\n    \"        \\n\",\\n    \"        # Standard deviation for the action distribution\\n\",\\n    \"        self.actor_logstd = nn.Parameter(torch.zeros(1, action_dim))\\n\",\\n    \"\\n\",\\n    \"    def get_value(self, x):\\n\",\\n    \"        return self.critic(x)\\n\",\\n    \"\\n\",\\n    \"    def get_action_and_value(self, x, action=None):\\n\",\\n    \"        \"\"\"\\n\",\\n    \"        Gets an action (and its log_prob) and the state value.\\n\",\\n    \"        If action is provided, it evaluates that action.\\n\",\\n    \"        If action is None, it samples a new action.\\n\",\\n    \"        \"\"\"\\n\",\\n    \"        # Actor output is the mean of a distribution in unbounded space\\n\",\\n    \"        action_mean = self.actor(x)\\n\",\\n    \"        \\n\",\\n    \"        action_logstd = self.actor_logstd.expand_as(action_mean)\\n\",\\n    \"        action_std = torch.exp(action_logstd)\\n\",\\n    \"        \\n\",\\n    \"        probs = Normal(action_mean, action_std)\\n\",\\n    \"        \\n\",\\n    \"        if action is None:\\n\",\\n    \"            # Sample new action from the unbounded distribution\\n\",\\n    \"            action_unbounded = probs.sample()\\n\",\\n    \"            # Squash to [-1, 1] using Tanh\\n\",\\n    \"            action_tanh = torch.tanh(action_unbounded)\\n\",\\n    \"            # Scale and shift to the correct action range\\n\",\\n    \"            action = self.action_bias + self.action_scale * action_tanh\\n\",\\n    \"        else:\\n\",\\n    \"            # Evaluate given action\\n\",\\n    \"            # We need to reverse the scaling to get the \"tanh\" value\\n\",\\n    \"            action_tanh = (action - self.action_bias) / self.action_scale\\n\",\\n    \"            # Clip to avoid numerical issues at the bounds\\n\",\\n    \"            action_tanh = torch.clamp(action_tanh, -0.9999, 0.9999)\\n\",\\n    \"            # Reverse the Tanh to get the unbounded action\\n\",\\n    \"            action_unbounded = torch.atanh(action_tanh)\\n\",\\n    \"            \\n\",\\n    \"        # Log-prob of the scaled action\\n\",\\n    \"        log_prob = probs.log_prob(action_unbounded)\\n\",\\n    \"        log_prob -= torch.log(self.action_scale * (1 - action_tanh.pow(2)) + 1e-6)\\n\",\\n    \"        log_prob = log_prob.sum(1, keepdim=True)\\n\",\\n    \"        \\n\",\\n    \"        entropy = probs.entropy().sum(1)\\n\",\\n    \"        value = self.critic(x)\\n\",\\n    \"        \\n\",\\n    \"        return action, log_prob, entropy, value\\n\",\\n    \"\\n\",\\n    \"def main():\\n\",\\n    \"    # Set device\\n\",\\n    \"    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\",\\n    \"    print(f\"Using device: {device}\")\\n\",\\n    \"    \\n\",\\n    \"    print(\"Initializing Go2 RL Environment...\")\\n\",\\n    \"    env = Go2Env()\\n\",\\n    \"    \\n\",\\n    \"    obs_dim = env.observation_space.shape[0]\\n\",\\n    \"    action_dim = env.action_space.shape[0]\\n\",\\n    \"    \\n\",\\n    \"    print(f\"Observation space dim: {obs_dim}\")\\n\",\\n    \"    print(f\"Action space dim: {action_dim}\")\\n\",\\n    \"\\n\",\\n    \"    # --- PPO Agent ---\\n\",\\n    \"    agent = ActorCritic(obs_dim, action_dim, env.action_low, env.action_high).to(device)\\n\",\\n    \"    optimizer = optim.Adam(agent.parameters(), lr=config.PPO_LEARNING_RATE, eps=1e-5)\\n\",\\n    \"    \\n\",\\n    \"    # --- Storage ---\\n\",\\n    \"    num_steps = config.PPO_STEPS_PER_EPOCH\\n\",\\n    \"    \\n\",\\n    \"    obs = torch.zeros((num_steps, obs_dim)).to(device)\\n\",\\n    \"    actions = torch.zeros((num_steps, action_dim)).to(device)\\n\",\\n    \"    logprobs = torch.zeros(num_steps).to(device)\\n\",\\n    \"    rewards = torch.zeros(num_steps).to(device)\\n\",\\n    \"    dones = torch.zeros(num_steps).to(device)\\n\",\\n    \"    values = torch.zeros(num_steps).to(device)\\n\",\\n    \"\\n\",\\n    \"    print(\"Starting PPO Training...\")\\n\",\\n    \"    start_time = time.time()\\n\",\\n    \"    \\n\",\\n    \"    next_obs, _ = env.reset()\\n\",\\n    \"    next_obs = torch.Tensor(next_obs).to(device)\\n\",\\n    \"    next_done = torch.zeros(1).to(device)\\n\",\\n    \"    \\n\",\\n    \"    for epoch in range(config.PPO_EPOCHS):\\n\",\\n    \"        epoch_rewards = []\\n\",\\n    \"        epoch_reward_components = {}\\n\",\\n    \"\\n\",\\n    \"        for step in range(num_steps):\\n\",\\n    \"            obs[step] = next_obs\\n\",\\n    \"            dones[step] = next_done\\n\",\\n    \"\\n\",\\n    \"            # Get action and value from agent\\n\",\\n    \"            with torch.no_grad():\\n\",\\n    \"                action, logprob, _, value = agent.get_action_and_value(next_obs.unsqueeze(0))\\n\",\\n    \"                values[step] = value.flatten()\\n\",\\n    \"            \\n\",\\n    \"            actions[step] = action.squeeze(0)\\n\",\\n    \"            logprobs[step] = logprob.squeeze()\\n\",\\n    \"\\n\",\\n    \"            # Step the environment\\n\",\\n    \"            next_obs_np, reward, terminated, truncated, info = env.step(action.cpu().numpy().squeeze(0))\\n\",\\n    \"            epoch_rewards.append(reward)\\n\",\\n    \"            \\n\",\\n    \"            # Log reward components\\n\",\\n    \"            for key, val in info.items():\\n\",\\n    \"                if key not in epoch_reward_components:\\n\",\\n    \"                    epoch_reward_components[key] = []\\n\",\\n    \"                epoch_reward_components[key].append(val)\\n\",\\n    \"            \\n\",\\n    \"            rewards[step] = torch.tensor(reward, device=device).view(-1)\\n\",\\n    \"            next_obs = torch.Tensor(next_obs_np).to(device)\\n\",\\n    \"            next_done = torch.tensor(float(terminated or truncated), device=device)\\n\",\\n    \"            \\n\",\\n    \"            if next_done:\\n\",\\n    \"                epoch_rewards = []\\n\",\\n    \"                epoch_reward_components = {}\\n\",\\n    \"                next_obs, _ = env.reset()\\n\",\\n    \"                next_obs = torch.Tensor(next_obs).to(device)\\n\",\\n    \"\\n\",\\n    \"        # --- Calculate Advantages (GAE) ---\\n\",\\n    \"        with torch.no_grad():\\n\",\\n    \"            next_value = agent.get_value(next_obs.unsqueeze(0)).reshape(1, -1)\\n\",\\n    \"            advantages = torch.zeros_like(rewards).to(device)\\n\",\\n    \"            lastgaelam = 0\\n\",\\n    \"            for t in reversed(range(num_steps)):\\n\",\\n    \"                if t == num_steps - 1:\\n\",\\n    \"                    nextnonterminal = 1.0 - next_done\\n\",\\n    \"                    nextvalues = next_value\\n\",\\n    \"                else:\\n\",\\n    \"                    nextnonterminal = 1.0 - dones[t + 1]\\n\",\\n    \"                    nextvalues = values[t + 1]\\n\",\\n    \"                \\n\",\\n    \"                delta = rewards[t] + config.PPO_GAMMA * nextvalues * nextnonterminal - values[t]\\n\",\\n    \"                advantages[t] = lastgaelam = delta + config.PPO_GAMMA * config.PPO_LAM * nextnonterminal * lastgaelam\\n\",\\n    \"            returns = advantages + values\\n\",\\n    \"\\n\",\\n    \"        # --- Update Policy ---\\n\",\\n    \"        b_obs = obs.reshape((-1,) + env.observation_space.shape)\\n\",\\n    \"        b_actions = actions.reshape((-1,) + env.action_space.shape)\\n\",\\n    \"        b_logprobs = logprobs.reshape(-1)\\n\",\\n    \"        b_advantages = advantages.reshape(-1)\\n\",\\n    \"        b_returns = returns.reshape(-1)\\n\",\\n    \"\\n\",\\n    \"        # Normalize advantages\\n\",\\n    \"        b_advantages = (b_advantages - b_advantages.mean()) / (b_advantages.std() + 1e-8)\\n\",\\n    \"        \\n\",\\n    \"        b_inds = np.arange(num_steps)\\n\",\\n    \"        for _ in range(config.PPO_UPDATE_EPOCHS):\\n\",\\n    \"            np.random.shuffle(b_inds)\\n\",\\n    \"            for start in range(0, num_steps, config.PPO_MINIBATCH_SIZE):\\n\",\\n    \"                end = start + config.PPO_MINIBATCH_SIZE\\n\",\\n    \"                mb_inds = b_inds[start:end]\\n\",\\n    \"\\n\",\\n    \"                _, newlogprob, entropy, newvalue = agent.get_action_and_value(\\n\",\\n    \"                    b_obs[mb_inds], b_actions[mb_inds]\\n\",\\n    \"                )\\n\",\\n    \"                logratio = newlogprob.squeeze() - b_logprobs[mb_inds]\\n\",\\n    \"                ratio = logratio.exp()\\n\",\\n    \"\\n\",\\n    \"                # Policy loss\\n\",\\n    \"                pg_loss1 = -b_advantages[mb_inds] * ratio\\n\",\\n    \"                pg_loss2 = -b_advantages[mb_inds] * torch.clamp(\\n\",\\n    \"                    ratio, 1 - config.PPO_CLIP, 1 + config.PPO_CLIP\\n\",\\n    \"                )\\n\",\\n    \"                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\\n\",\\n    \"\\n\",\\n    \"                # Value loss\\n\",\\n    \"                v_loss = 0.5 * ((newvalue.squeeze() - b_returns[mb_inds]) ** 2).mean()\\n\",\\n    \"\\n\",\\n    \"                # Entropy loss\\n\",\\n    \"                entropy_loss = entropy.mean()\\n\",\\n    \"\\n\",\\n    \"                # Total loss\\n\",\\n    \"                loss = pg_loss - 0.01 * entropy_loss + v_loss * 0.5\\n\",\\n    \"\\n\",\\n    \"                optimizer.zero_grad()\\n\",\\n    \"                loss.backward()\\n\",\\n    \"                nn.utils.clip_grad_norm_(agent.parameters(), 0.5)\\n\",\\n    \"                optimizer.step()\\n\",\\n    \"\\n\",\\n    \"        # --- Logging ---\\n\",\\n    \"        num_episodes = dones.sum().item()\\n\",\\n    \"        if num_episodes == 0:\\n\",\\n    \"            avg_reward = np.nan # Avoid division by zero if no episodes finished\\n\",\\n    \"        else:\\n\",\\n    \"            avg_reward = rewards.sum().item() / num_episodes\\n\",\\n    \"            \\n\",\\n    \"        print(f\"Epoch {epoch+1}/{config.PPO_EPOCHS} | Avg. Ep Reward: {avg_reward:.2f} | Time: {time.time()-start_time:.2f}s\")\\n\",\\n    \"        \\n\",\\n    \"        # Log mean of reward components\\n\",\\n    \"        for key, val_list in epoch_reward_components.items():\\n\",\\n    \"            if val_list:\\n\",\\n    \"                print(f\"  ... avg {key}: {np.mean(val_list):.3f}\")\\n\",\\n    \"        \\n\",\\n    \"    env.close()\\n\",\\n    \"    print(\"Training finished.\")\\n\",\\n    \"    \\n\",\\n    \"    # Save the trained policy\\n\",\\n    \"    model_path = \"ppo_go2_policy.pth\"\\n\",\\n    \"    torch.save(agent.state_dict(), model_path)\\n\",\\n    \"    print(f\"Trained policy saved to {model_path}\")\\n\",\\n    \"\\n\",\\n    \"if __name__ == \"__main__\":\\n\",\\n    \"    main()\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"## Step 4: Run Training\\n\",\\n    \"\\n\",\\n    \"This final cell executes the training script. It will print the average reward for each epoch. Training will take a while!\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"!python train_ppo.py\"\\n   ]\\n  }\\n ],\\n \"metadata\": {\\n  \"kernelspec\": {\\n   \"display_name\": \"Python 3\",\\n   \"language\": \"python\",\\n   \"name\": \"python3\"\\n  },\\n  \"language_info\": {\\n   \"codemirror_mode\": {\\n    \"name\": \"ipython\",\\n    \"version\": 3\\n   },\\n   \"file_extension\": \".py\",\\n   \"mimetype\": \"text/x-python\",\\n   \"name\": \"python\",\\n   \"nbconvert_exporter\": \"python\",\\n   \"pygments_lexer\": \"ipython3\",\\n   \"version\": \"3.10.12\"\\n  }\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 4\\n}\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f6f6781"
      },
      "source": [
        "# Go2 Quadruped RL Trainer (MuJoCo + PPO)\n",
        "\n",
        "This notebook will set up a complete environment for training the Unitree Go2 robot to walk using Reinforcement Learning (PPO).\n",
        "\n",
        "**Instructions:**\n",
        "1.  Make sure your runtime is set to use a GPU (Runtime > Change runtime type > T4 GPU) for faster training.\n",
        "2.  Run the cells in order from top to bottom."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86fc9d20"
      },
      "source": [
        "## Step 1: Install Dependencies\n",
        "\n",
        "This cell installs all necessary Python libraries for the simulation and RL algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7716d328"
      },
      "source": [
        "!pip install -q mujoco gymnasium torch scipy"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "382e4db4"
      },
      "source": [
        "## Step 2: Get Robot Model and Assets\n",
        "\n",
        "We clone the official `unitree_mujoco` repository. This gives us the `go2.xml` file and, most importantly, the `assets/` folder containing all the `.obj` mesh files the robot model needs to load."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd1dabc9",
        "outputId": "b51956a1-4308-46a0-fe50-cdc404d8a12f"
      },
      "source": [
        "!git clone https://github.com/unitreerobotics/unitree_mujoco"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'unitree_mujoco'...\n",
            "remote: Enumerating objects: 729, done.\u001b[K\n",
            "remote: Counting objects: 100% (235/235), done.\u001b[K\n",
            "remote: Compressing objects: 100% (111/111), done.\u001b[K\n",
            "remote: Total 729 (delta 163), reused 131 (delta 124), pack-reused 494 (from 1)\u001b[K\n",
            "Receiving objects: 100% (729/729), 62.56 MiB | 17.47 MiB/s, done.\n",
            "Resolving deltas: 100% (264/264), done.\n",
            "Updating files: 100% (390/390), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4c12eef"
      },
      "source": [
        "## Step 3: Create Python Training Files\n",
        "\n",
        "We use the `%%writefile` magic command to create our four Python scripts in the Colab environment's main directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d4c084d",
        "outputId": "df64374b-1fcb-45ea-e7b9-b0db79021233"
      },
      "source": [
        "%%writefile config.py\n",
        "import numpy as np\n",
        "\n",
        "# --- Simulation ---\n",
        "# This path now points inside the cloned repository\n",
        "XML_PATH = 'unitree_mujoco/unitree_robots/go2/go2.xml'\n",
        "\n",
        "SIM_HZ = 500  # (Hz) Frequency of MuJoCo physics steps\n",
        "CONTROL_HZ = 50  # (Hz) Frequency of RL agent policy decisions\n",
        "FRAME_SKIP = SIM_HZ // CONTROL_HZ # Number of physics steps per agent step\n",
        "MAX_EPISODE_STEPS = 1000  # Max steps before env reset\n",
        "\n",
        "# --- Robot Model ---\n",
        "# These names are taken from the official 'go2.xml'\n",
        "TRUNK_BODY_NAME = \"trunk\"\n",
        "JOINT_NAMES = [\n",
        "    \"FR_hip_joint\", \"FR_thigh_joint\", \"FR_calf_joint\",\n",
        "    \"FL_hip_joint\", \"FL_thigh_joint\", \"FL_calf_joint\",\n",
        "    \"RR_hip_joint\", \"RR_thigh_joint\", \"RR_calf_joint\",\n",
        "    \"RL_hip_joint\", \"RL_thigh_joint\", \"RL_calf_joint\",\n",
        "]\n",
        "# We use the calf bodies to check for contact, as they contain the foot geoms\n",
        "FOOT_BODY_NAMES = [\"FR_calf\", \"FL_calf\", \"RR_calf\", \"RL_calf\"]\n",
        "\n",
        "# --- Locomotion ---\n",
        "TARGET_VELOCITY = 0.8  # (m/s) Target forward velocity (x-axis)\n",
        "TARGET_HEIGHT = 0.3    # (m) Target CoM height\n",
        "CONTACT_FORCE_THRESHOLD = 5.0  # (N) Force threshold to register foot contact\n",
        "\n",
        "# --- RL Reward Weights ---\n",
        "W_VEL_X = 2.0         # Reward for matching target x-velocity\n",
        "W_VEL_Y = -1.0        # Penalty for y-velocity\n",
        "W_VEL_Z = -1.0        # Penalty for z-velocity\n",
        "W_ANG_VEL = -0.1      # Penalty for angular velocity\n",
        "W_COM_HEIGHT = 1.5    # Reward for maintaining target height\n",
        "W_ORIENTATION = -2.0  # Penalty for roll and pitch\n",
        "W_ACTION_RATE = -0.01 # Penalty for jerky actions\n",
        "W_TORQUE = -0.00002   # Penalty for motor effort (torques)\n",
        "W_CONTACT_FORCE = -0.0001 # Penalty for high contact forces\n",
        "W_COM_IN_SUPPORT = 3.0 # Reward for keeping CoM in support polygon\n",
        "W_FALL = -200.0       # Large penalty for falling\n",
        "\n",
        "# --- PPO Training ---\n",
        "PPO_STEPS_PER_EPOCH = 4096\n",
        "PPO_EPOCHS = 500\n",
        "PPO_LEARNING_RATE = 3e-4\n",
        "PPO_MINIBATCH_SIZE = 64\n",
        "PPO_UPDATE_EPOCHS = 10\n",
        "PPO_GAMMA = 0.99\n",
        "PPO_LAM = 0.95\n",
        "PPO_CLIP = 0.2"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b691877b",
        "outputId": "c5e8c2e5-f09f-466a-b5eb-fd8c35cc5e48"
      },
      "source": [
        "%%writefile utils.py\n",
        "import numpy as np\n",
        "import mujoco\n",
        "from scipy.spatial import ConvexHull, Delaunay\n",
        "\n",
        "# --- MuJoCo Model/Data Getters ---\n",
        "\n",
        "def get_body_id(model, body_name):\n",
        "    \"\"\"Returns the MuJoCo ID for a body.\"\"\"\n",
        "    return mujoco.mj_name2id(model, mujoco.mjtObj.mjOBJ_BODY, body_name)\n",
        "\n",
        "def get_joint_qpos_ids(model, joint_names):\n",
        "    \"\"\"Returns qpos indices for a list of joint names.\"\"\"\n",
        "    return [model.jnt_qposadr[mujoco.mj_name2id(model, mujoco.mjtObj.mjOBJ_JOINT, name)] for name in joint_names]\n",
        "\n",
        "def get_joint_qvel_ids(model, joint_names):\n",
        "    \"\"\"Returns qvel indices for a list of joint names.\"\"\"\n",
        "    return [model.jnt_dofadr[mujoco.mj_name2id(model, mujoco.mjtObj.mjOBJ_JOINT, name)] for name in joint_names]\n",
        "\n",
        "def get_actuator_ids(model, joint_names):\n",
        "    \"\"\"Returns actuator indices for a list of joint names.\"\"\"\n",
        "    # Assumes motor name is joint name + \"_motor\"\n",
        "    return [mujoco.mj_name2id(model, mujoco.mjtObj.mjOBJ_ACTUATOR, name.replace(\"_joint\", \"_motor\")) for name in joint_names]\n",
        "\n",
        "def get_foot_body_ids(model, foot_body_names):\n",
        "    \"\"\"Returns body IDs for a list of foot body names.\"\"\"\n",
        "    return [get_body_id(model, name) for name in foot_body_names]\n",
        "\n",
        "# --- Kinematics & Dynamics ---\n",
        "\n",
        "def get_com_position(data, trunk_id):\n",
        "    \"\"\"Returns the 3D position of the trunk (CoM).\"\"\"\n",
        "    return data.xpos[trunk_id]\n",
        "\n",
        "def get_com_velocity(data, trunk_id):\n",
        "    \"\"\"Returns the 3D linear velocity of the trunk (CoM).\"\"\"\n",
        "    # Use cvel (velocities in world frame)\n",
        "    return data.cvel[trunk_id, 3:6]\n",
        "\n",
        "def get_body_orientation(data, trunk_id):\n",
        "    \"\"\"Returns the quaternion orientation of the trunk.\"\"\"\n",
        "    return data.xquat[trunk_id]\n",
        "\n",
        "def get_body_angular_velocity(data, trunk_id):\n",
        "    \"\"\"Returns the 3D angular velocity of the trunk.\"\"\"\n",
        "    # Use cvel (velocities in world frame)\n",
        "    return data.cvel[trunk_id, 0:3]\n",
        "\n",
        "def quat_to_rpy(quat):\n",
        "    \"\"\"Converts a quaternion (w, x, y, z) to roll, pitch, yaw.\"\"\"\n",
        "    w, x, y, z = quat\n",
        "\n",
        "    # Roll (x-axis rotation)\n",
        "    sinr_cosp = 2 * (w * x + y * z)\n",
        "    cosr_cosp = 1 - 2 * (x * x + y * y)\n",
        "    roll = np.arctan2(sinr_cosp, cosr_cosp)\n",
        "\n",
        "    # Pitch (y-axis rotation)\n",
        "    sinp = 2 * (w * y - z * x)\n",
        "    if np.abs(sinp) >= 1:\n",
        "        pitch = np.copysign(np.pi / 2, sinp)  # Use 90 degrees if out of range\n",
        "    else:\n",
        "        pitch = np.arcsin(sinp)\n",
        "\n",
        "    # Yaw (z-axis rotation)\n",
        "    siny_cosp = 2 * (w * z + x * y)\n",
        "    cosy_cosp = 1 - 2 * (y * y + z * z)\n",
        "    yaw = np.arctan2(siny_cosp, cosy_cosp)\n",
        "\n",
        "    return roll, pitch, yaw\n",
        "\n",
        "# --- Contact & Stability ---\n",
        "\n",
        "def get_foot_contacts(model, data, foot_body_ids, contact_force_threshold):\n",
        "    \"\"\"\n",
        "    Checks for foot contact with the ground.\n",
        "    Returns a boolean array [FR, FL, RR, RL]\n",
        "    \"\"\"\n",
        "    contacts = [False] * 4\n",
        "    for i in range(data.ncon):\n",
        "        contact = data.contact[i]\n",
        "\n",
        "        # Check if geom1 or geom2 is a foot\n",
        "        geom1_body = model.geom_bodyid[contact.geom1]\n",
        "        geom2_body = model.geom_bodyid[contact.geom2]\n",
        "\n",
        "        is_geom1_foot = geom1_body in foot_body_ids\n",
        "        is_geom2_foot = geom2_body in foot_body_ids\n",
        "\n",
        "        if not (is_geom1_foot or is_geom2_foot):\n",
        "            continue # Not a foot contact\n",
        "\n",
        "        # Check if the other geom is the ground (geom ID 0)\n",
        "        is_geom1_ground = contact.geom1 == 0\n",
        "        is_geom2_ground = contact.geom2 == 0\n",
        "\n",
        "        if not (is_geom1_ground or is_geom2_ground):\n",
        "            continue # Not a ground contact\n",
        "\n",
        "        # Get contact force\n",
        "        force_normal = np.zeros(3)\n",
        "        mujoco.mj_contactForce(model, data, i, force_normal)\n",
        "\n",
        "        if np.linalg.norm(force_normal) > contact_force_threshold:\n",
        "            if is_geom1_foot:\n",
        "                foot_idx = foot_body_ids.index(geom1_body)\n",
        "                contacts[foot_idx] = True\n",
        "            if is_geom2_foot:\n",
        "                foot_idx = foot_body_ids.index(geom2_body)\n",
        "                contacts[foot_idx] = True\n",
        "\n",
        "    return np.array(contacts)\n",
        "\n",
        "def get_foot_positions(data, foot_body_ids):\n",
        "    \"\"\"Returns the 3D world positions of the feet (calf bodies).\"\"\"\n",
        "    return data.xpos[foot_body_ids]\n",
        "\n",
        "def get_support_polygon(foot_positions, contact_states):\n",
        "    \"\"\"\n",
        "    Returns the 2D vertices (x, y) of the support polygon.\n",
        "    Returns an empty list if fewer than 2 feet are in contact.\n",
        "    \"\"\"\n",
        "    stance_feet_pos = foot_positions[contact_states, :2] # Get (x, y) of stance feet\n",
        "\n",
        "    if stance_feet_pos.shape[0] < 2:\n",
        "        return [] # Not enough points to form a polygon\n",
        "\n",
        "    if stance_feet_pos.shape[0] == 2:\n",
        "        return stance_feet_pos # Support polygon is a line\n",
        "\n",
        "    try:\n",
        "        # A Bounding Box is simpler and more stable than Convex Hull for 3 points\n",
        "        if stance_feet_pos.shape[0] == 3:\n",
        "             return stance_feet_pos\n",
        "\n",
        "        hull = ConvexHull(stance_feet_pos)\n",
        "        return stance_feet_pos[hull.vertices]\n",
        "    except Exception:\n",
        "        return [] # Error during hull calculation (e.g., colinear points)\n",
        "\n",
        "def is_com_stable(com_pos_2d, support_polygon):\n",
        "    \"\"\"\n",
        "    Checks if the 2D CoM position is inside the 2D support polygon.\n",
        "    Uses scipy.spatial.Delaunay for robust point-in-polygon check.\n",
        "    \"\"\"\n",
        "    if len(support_polygon) < 3:\n",
        "        # If support is a line (2 feet) or point (1 foot),\n",
        "        # we can't use a polygon check.\n",
        "        # For simplicity, we'll call it \"unstable\"\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Create a Delaunay triangulation of the support polygon\n",
        "        hull = Delaunay(support_polygon)\n",
        "\n",
        "        # find_simplex returns -1 if the point is outside the hull\n",
        "        return hull.find_simplex(com_pos_2d) >= 0\n",
        "    except Exception:\n",
        "        # Error (e.g., flat polygon)\n",
        "        return False"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb804cdc",
        "outputId": "dcb8e72f-2451-4047-bd62-fc7b915621ec"
      },
      "source": [
        "%%writefile go2_env.py\n",
        "import mujoco\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import config\n",
        "import utils\n",
        "\n",
        "class Go2Env(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Gymnasium environment for the Unitree Go2 robot using MuJoCo.\n",
        "    \"\"\"\n",
        "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 50}\n",
        "\n",
        "    def __init__(self, render_mode=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        # Construct the full path to the XML file\n",
        "        # This path is relative to where the script is run (e.g., /content/)\n",
        "        xml_path = config.XML_PATH\n",
        "\n",
        "        if not os.path.exists(xml_path):\n",
        "            raise FileNotFoundError(\n",
        "                f\"Could not find XML file: {xml_path}. \"\n",
        "                f\"Make sure the 'unitree_mujoco' repo was cloned successfully.\"\n",
        "            )\n",
        "\n",
        "        self.model = mujoco.MjModel.from_xml_path(xml_path)\n",
        "        self.data = mujoco.MjData(self.model)\n",
        "\n",
        "        # --- Get element IDs from model ---\n",
        "        self.trunk_id = utils.get_body_id(self.model, config.TRUNK_BODY_NAME)\n",
        "        self.joint_qpos_ids = utils.get_joint_qpos_ids(self.model, config.JOINT_NAMES)\n",
        "        self.joint_qvel_ids = utils.get_joint_qvel_ids(self.model, config.JOINT_NAMES)\n",
        "        self.actuator_ids = utils.get_actuator_ids(self.model, config.JOINT_NAMES)\n",
        "        self.foot_body_ids = utils.get_foot_body_ids(self.model, config.FOOT_BODY_NAMES)\n",
        "\n",
        "        # Store initial state for resets\n",
        "        self.init_qpos = self.data.qpos.copy()\n",
        "        self.init_qvel = self.data.qvel.copy()\n",
        "        self.action_history = np.zeros(12)\n",
        "\n",
        "        # --- Define Action Space ---\n",
        "        act_dim = 12\n",
        "        ctrl_range = self.model.actuator_ctrlrange[self.actuator_ids]\n",
        "        self.action_low = ctrl_range[:, 0]\n",
        "        self.action_high = ctrl_range[:, 1]\n",
        "\n",
        "        self.action_space = spaces.Box(\n",
        "            low=self.action_low,\n",
        "            high=self.action_high,\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # --- Define Observation Space ---\n",
        "        obs_dim = 3 + 3 + 4 + 12 + 12 + 4 # com_vel, ang_vel, quat, qpos, qvel, contacts\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf,\n",
        "            high=np.inf,\n",
        "            shape=(obs_dim,),\n",
        "            dtype=np.float64\n",
        "        )\n",
        "\n",
        "        # --- Rendering (Colab) ---\n",
        "        # We'll use 'rgb_array' for Colab. 'human' mode won't work.\n",
        "        if self.render_mode == \"human\":\n",
        "            print(\"Warning: 'human' render_mode not supported in Colab. Use 'rgb_array' instead.\")\n",
        "            self.render_mode = \"rgb_array\"\n",
        "\n",
        "        if self.render_mode == \"rgb_array\":\n",
        "            self.renderer = mujoco.Renderer(self.model, 480, 640)\n",
        "        else:\n",
        "            self.renderer = None\n",
        "\n",
        "    def _get_obs(self):\n",
        "        \"\"\"Constructs the observation vector from simulation data.\"\"\"\n",
        "        com_vel = utils.get_com_velocity(self.data, self.trunk_id)\n",
        "        ang_vel = utils.get_body_angular_velocity(self.data, self.trunk_id)\n",
        "        quat = utils.get_body_orientation(self.data, self.trunk_id)\n",
        "        qpos = self.data.qpos[self.joint_qpos_ids]\n",
        "        qvel = self.data.qvel[self.joint_qvel_ids]\n",
        "        contacts = utils.get_foot_contacts(\n",
        "            self.model, self.data, self.foot_body_ids, config.CONTACT_FORCE_THRESHOLD\n",
        "        )\n",
        "\n",
        "        return np.concatenate([\n",
        "            com_vel, ang_vel, quat, qpos, qvel, contacts.astype(float)\n",
        "        ])\n",
        "\n",
        "    def _compute_reward(self, action):\n",
        "        \"\"\"Calculates the reward based on the current state and action.\"\"\"\n",
        "\n",
        "        # --- Get current state data ---\n",
        "        com_pos = utils.get_com_position(self.data, self.trunk_id)\n",
        "        com_vel = utils.get_com_velocity(self.data, self.trunk_id)\n",
        "        ang_vel = utils.get_body_angular_velocity(self.data, self.trunk_id)\n",
        "        quat = utils.get_body_orientation(self.data, self.trunk_id)\n",
        "        torques = self.data.actuator_force[self.actuator_ids]\n",
        "\n",
        "        # 1. Velocity Tracking (X, Y, Z)\n",
        "        vel_error_x = (com_vel[0] - config.TARGET_VELOCITY)**2\n",
        "        vel_error_y = com_vel[1]**2\n",
        "        vel_error_z = com_vel[2]**2\n",
        "\n",
        "        reward_vel_x = config.W_VEL_X * np.exp(-vel_error_x * 5.0)\n",
        "        penalty_vel_y = config.W_VEL_Y * vel_error_y\n",
        "        penalty_vel_z = config.W_VEL_Z * vel_error_z\n",
        "\n",
        "        # 2. CoM Height\n",
        "        height_error = (com_pos[2] - config.TARGET_HEIGHT)**2\n",
        "        reward_height = config.W_COM_HEIGHT * np.exp(-height_error * 20.0)\n",
        "\n",
        "        # 3. Orientation\n",
        "        roll, pitch, _ = utils.quat_to_rpy(quat)\n",
        "        orientation_penalty = config.W_ORIENTATION * (roll**2 + pitch**2)\n",
        "        ang_vel_penalty = config.W_ANG_VEL * np.sum(ang_vel**2)\n",
        "\n",
        "        # 4. Effort / Torque / Action Rate\n",
        "        torque_penalty = config.W_TORQUE * np.sum(torques**2)\n",
        "        action_rate_penalty = config.W_ACTION_RATE * np.sum((action - self.action_history)**2)\n",
        "        self.action_history = action # store for next step\n",
        "\n",
        "        # 5. Stability: CoM within Support Polygon\n",
        "        contacts = utils.get_foot_contacts(\n",
        "            self.model, self.data, self.foot_body_ids, config.CONTACT_FORCE_THRESHOLD\n",
        "        )\n",
        "        foot_positions = utils.get_foot_positions(self.data, self.foot_body_ids)\n",
        "        support_polygon = utils.get_support_polygon(foot_positions, contacts)\n",
        "\n",
        "        is_stable = utils.is_com_stable(com_pos[:2], support_polygon)\n",
        "        reward_com_stable = config.W_COM_IN_SUPPORT if is_stable else 0.0\n",
        "\n",
        "        # --- Sum Rewards ---\n",
        "        total_reward = (\n",
        "            reward_vel_x + penalty_vel_y + penalty_vel_z +\n",
        "            reward_height +\n",
        "            orientation_penalty + ang_vel_penalty +\n",
        "            torque_penalty + action_rate_penalty +\n",
        "            reward_com_stable\n",
        "        )\n",
        "\n",
        "        reward_info = {\n",
        "            \"r_vel_x\": reward_vel_x,\n",
        "            \"p_vel_y\": penalty_vel_y,\n",
        "            \"p_vel_z\": penalty_vel_z,\n",
        "            \"r_height\": reward_height,\n",
        "            \"p_orientation\": orientation_penalty,\n",
        "            \"p_ang_vel\": ang_vel_penalty,\n",
        "            \"p_torque\": torque_penalty,\n",
        "            \"p_action_rate\": action_rate_penalty,\n",
        "            \"r_com_stable\": reward_com_stable,\n",
        "        }\n",
        "\n",
        "        return total_reward, reward_info\n",
        "\n",
        "    def _check_termination(self):\n",
        "        \"\"\"Checks if the episode should terminate (e.g., robot fell).\"\"\"\n",
        "        com_pos = utils.get_com_position(self.data, self.trunk_id)\n",
        "        quat = utils.get_body_orientation(self.data, self.trunk_id)\n",
        "        roll, pitch, _ = utils.quat_to_rpy(quat)\n",
        "\n",
        "        # Fell if CoM is too low or if roll/pitch is too high\n",
        "        is_fallen = (com_pos[2] < 0.15) or (abs(roll) > 1.0) or (abs(pitch) > 1.0)\n",
        "\n",
        "        return is_fallen\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Run one timestep of the environment's dynamics.\"\"\"\n",
        "\n",
        "        # Clip action to be safe\n",
        "        action = np.clip(action, self.action_low, self.action_high)\n",
        "\n",
        "        # Set the target joint positions as the control signal\n",
        "        self.data.ctrl[self.actuator_ids] = action\n",
        "\n",
        "        # Step the simulation forward\n",
        "        mujoco.mj_step(self.model, self.data, nstep=config.FRAME_SKIP)\n",
        "\n",
        "        # Get new state, reward, and termination status\n",
        "        observation = self._get_obs()\n",
        "        terminated = self._check_termination()\n",
        "        reward, reward_info = self._compute_reward(action)\n",
        "\n",
        "        if terminated:\n",
        "            reward += config.W_FALL # Add large fall penalty\n",
        "\n",
        "        self.step_count += 1\n",
        "        truncated = self.step_count >= config.MAX_EPISODE_STEPS\n",
        "\n",
        "        return observation, reward, terminated, truncated, reward_info\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        \"\"\"Reset the environment to an initial state.\"\"\"\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        self.step_count = 0\n",
        "        self.action_history = np.zeros(12)\n",
        "\n",
        "        mujoco.mj_resetData(self.model, self.data)\n",
        "        self.data.qpos[:] = self.init_qpos\n",
        "        self.data.qvel[:] = self.init_qvel\n",
        "\n",
        "        # Add small random noise to initial joint positions\n",
        "        self.data.qpos[self.joint_qpos_ids] += self.np_random.uniform(\n",
        "            -0.1, 0.1, size=len(self.joint_qpos_ids)\n",
        "        )\n",
        "\n",
        "        mujoco.mj_forward(self.model, self.data)\n",
        "\n",
        "        observation = self._get_obs()\n",
        "        return observation, {}\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Render the environment (if in 'rgb_array' mode).\"\"\"\n",
        "        if self.renderer:\n",
        "            self.renderer.update_scene(self.data)\n",
        "            return self.renderer.render()\n",
        "        return None\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close the viewer.\"\"\"\n",
        "        if self.renderer:\n",
        "            self.renderer = None"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing go2_env.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f047c98b",
        "outputId": "b0ce3a22-7ebc-43cf-9c5d-353a8e7c4a71"
      },
      "source": [
        "%%writefile train_ppo.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.normal import Normal\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "from go2_env import Go2Env  # Import the custom environment\n",
        "import config  # Import config file\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    \"\"\"PPO Actor-Critic network.\"\"\"\n",
        "    def __init__(self, obs_dim, action_dim, action_low, action_high):\n",
        "        super().__init__()\n",
        "\n",
        "        # Store action scaling parameters\n",
        "        self.action_low = torch.tensor(action_low, dtype=torch.float32)\n",
        "        self.action_high = torch.tensor(action_high, dtype=torch.float32)\n",
        "        self.action_scale = (self.action_high - self.action_low) / 2.0\n",
        "        self.action_bias = (self.action_high + self.action_low) / 2.0\n",
        "\n",
        "        hidden_dim = 256\n",
        "\n",
        "        # Critic network\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "        # Actor network\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "        # Standard deviation for the action distribution\n",
        "        self.actor_logstd = nn.Parameter(torch.zeros(1, action_dim))\n",
        "\n",
        "    def get_value(self, x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        \"\"\"\n",
        "        Gets an action (and its log_prob) and the state value.\n",
        "        If action is provided, it evaluates that action.\n",
        "        If action is None, it samples a new action.\n",
        "        \"\"\"\n",
        "        # Actor output is the mean of a distribution in unbounded space\n",
        "        action_mean = self.actor(x)\n",
        "\n",
        "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
        "        action_std = torch.exp(action_logstd)\n",
        "\n",
        "        probs = Normal(action_mean, action_std)\n",
        "\n",
        "        if action is None:\n",
        "            # Sample new action from the unbounded distribution\n",
        "            action_unbounded = probs.sample()\n",
        "            # Squash to [-1, 1] using Tanh\n",
        "            action_tanh = torch.tanh(action_unbounded)\n",
        "            # Scale and shift to the correct action range\n",
        "            action = self.action_bias + self.action_scale * action_tanh\n",
        "        else:\n",
        "            # Evaluate given action\n",
        "            # We need to reverse the scaling to get the \"tanh\" value\n",
        "            action_tanh = (action - self.action_bias) / self.action_scale\n",
        "            # Clip to avoid numerical issues at the bounds\n",
        "            action_tanh = torch.clamp(action_tanh, -0.9999, 0.9999)\n",
        "            # Reverse the Tanh to get the unbounded action\n",
        "            action_unbounded = torch.atanh(action_tanh)\n",
        "\n",
        "        # Log-prob of the scaled action\n",
        "        log_prob = probs.log_prob(action_unbounded)\n",
        "        log_prob -= torch.log(self.action_scale * (1 - action_tanh.pow(2)) + 1e-6)\n",
        "        log_prob = log_prob.sum(1, keepdim=True)\n",
        "\n",
        "        entropy = probs.entropy().sum(1)\n",
        "        value = self.critic(x)\n",
        "\n",
        "        return action, log_prob, entropy, value\n",
        "\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    print(\"Initializing Go2 RL Environment...\")\n",
        "    env = Go2Env()\n",
        "\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "\n",
        "    print(f\"Observation space dim: {obs_dim}\")\n",
        "    print(f\"Action space dim: {action_dim}\")\n",
        "\n",
        "    # --- PPO Agent ---\n",
        "    agent = ActorCritic(obs_dim, action_dim, env.action_low, env.action_high).to(device)\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=config.PPO_LEARNING_RATE, eps=1e-5)\n",
        "\n",
        "    # --- Storage ---\n",
        "    num_steps = config.PPO_STEPS_PER_EPOCH\n",
        "\n",
        "    obs = torch.zeros((num_steps, obs_dim)).to(device)\n",
        "    actions = torch.zeros((num_steps, action_dim)).to(device)\n",
        "    logprobs = torch.zeros(num_steps).to(device)\n",
        "    rewards = torch.zeros(num_steps).to(device)\n",
        "    dones = torch.zeros(num_steps).to(device)\n",
        "    values = torch.zeros(num_steps).to(device)\n",
        "\n",
        "    print(\"Starting PPO Training...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    next_obs, _ = env.reset()\n",
        "    next_obs = torch.Tensor(next_obs).to(device)\n",
        "    next_done = torch.zeros(1).to(device)\n",
        "\n",
        "    for epoch in range(config.PPO_EPOCHS):\n",
        "        epoch_rewards = []\n",
        "        epoch_reward_components = {}\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            obs[step] = next_obs\n",
        "            dones[step] = next_done\n",
        "\n",
        "            # Get action and value from agent\n",
        "            with torch.no_grad():\n",
        "                action, logprob, _, value = agent.get_action_and_value(next_obs.unsqueeze(0))\n",
        "                values[step] = value.flatten()\n",
        "\n",
        "            actions[step] = action.squeeze(0)\n",
        "            logprobs[step] = logprob.squeeze()\n",
        "\n",
        "            # Step the environment\n",
        "            next_obs_np, reward, terminated, truncated, info = env.step(action.cpu().numpy().squeeze(0))\n",
        "            epoch_rewards.append(reward)\n",
        "\n",
        "            # Log reward components\n",
        "            for key, val in info.items():\n",
        "                if key not in epoch_reward_components:\n",
        "                    epoch_reward_components[key] = []\n",
        "                epoch_reward_components[key].append(val)\n",
        "\n",
        "            rewards[step] = torch.tensor(reward, device=device).view(-1)\n",
        "            next_obs = torch.Tensor(next_obs_np).to(device)\n",
        "            next_done = torch.tensor(float(terminated or truncated), device=device)\n",
        "\n",
        "            if next_done:\n",
        "                epoch_rewards = []\n",
        "                epoch_reward_components = {}\n",
        "                next_obs, _ = env.reset()\n",
        "                next_obs = torch.Tensor(next_obs).to(device)\n",
        "\n",
        "        # --- Calculate Advantages (GAE) ---\n",
        "        with torch.no_grad():\n",
        "            next_value = agent.get_value(next_obs.unsqueeze(0)).reshape(1, -1)\n",
        "            advantages = torch.zeros_like(rewards).to(device)\n",
        "            lastgaelam = 0\n",
        "            for t in reversed(range(num_steps)):\n",
        "                if t == num_steps - 1:\n",
        "                    nextnonterminal = 1.0 - next_done\n",
        "                    nextvalues = next_value\n",
        "                else:\n",
        "                    nextnonterminal = 1.0 - dones[t + 1]\n",
        "                    nextvalues = values[t + 1]\n",
        "\n",
        "                delta = rewards[t] + config.PPO_GAMMA * nextvalues * nextnonterminal - values[t]\n",
        "                advantages[t] = lastgaelam = delta + config.PPO_GAMMA * config.PPO_LAM * nextnonterminal * lastgaelam\n",
        "            returns = advantages + values\n",
        "\n",
        "        # --- Update Policy ---\n",
        "        b_obs = obs.reshape((-1,) + env.observation_space.shape)\n",
        "        b_actions = actions.reshape((-1,) + env.action_space.shape)\n",
        "        b_logprobs = logprobs.reshape(-1)\n",
        "        b_advantages = advantages.reshape(-1)\n",
        "        b_returns = returns.reshape(-1)\n",
        "\n",
        "        # Normalize advantages\n",
        "        b_advantages = (b_advantages - b_advantages.mean()) / (b_advantages.std() + 1e-8)\n",
        "\n",
        "        b_inds = np.arange(num_steps)\n",
        "        for _ in range(config.PPO_UPDATE_EPOCHS):\n",
        "            np.random.shuffle(b_inds)\n",
        "            for start in range(0, num_steps, config.PPO_MINIBATCH_SIZE):\n",
        "                end = start + config.PPO_MINIBATCH_SIZE\n",
        "                mb_inds = b_inds[start:end]\n",
        "\n",
        "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(\n",
        "                    b_obs[mb_inds], b_actions[mb_inds]\n",
        "                )\n",
        "                logratio = newlogprob.squeeze() - b_logprobs[mb_inds]\n",
        "                ratio = logratio.exp()\n",
        "\n",
        "                # Policy loss\n",
        "                pg_loss1 = -b_advantages[mb_inds] * ratio\n",
        "                pg_loss2 = -b_advantages[mb_inds] * torch.clamp(\n",
        "                    ratio, 1 - config.PPO_CLIP, 1 + config.PPO_CLIP\n",
        "                )\n",
        "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "                # Value loss\n",
        "                v_loss = 0.5 * ((newvalue.squeeze() - b_returns[mb_inds]) ** 2).mean()\n",
        "\n",
        "                # Entropy loss\n",
        "                entropy_loss = entropy.mean()\n",
        "\n",
        "                # Total loss\n",
        "                loss = pg_loss - 0.01 * entropy_loss + v_loss * 0.5\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(agent.parameters(), 0.5)\n",
        "                optimizer.step()\n",
        "\n",
        "        # --- Logging ---\n",
        "        num_episodes = dones.sum().item()\n",
        "        if num_episodes == 0:\n",
        "            avg_reward = np.nan # Avoid division by zero if no episodes finished\n",
        "        else:\n",
        "            avg_reward = rewards.sum().item() / num_episodes\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config.PPO_EPOCHS} | Avg. Ep Reward: {avg_reward:.2f} | Time: {time.time()-start_time:.2f}s\")\n",
        "\n",
        "        # Log mean of reward components\n",
        "        for key, val_list in epoch_reward_components.items():\n",
        "            if val_list:\n",
        "                print(f\"  ... avg {key}: {np.mean(val_list):.3f}\")\n",
        "\n",
        "    env.close()\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "    # Save the trained policy\n",
        "    model_path = \"ppo_go2_policy.pth\"\n",
        "    torch.save(agent.state_dict(), model_path)\n",
        "    print(f\"Trained policy saved to {model_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_ppo.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6032234"
      },
      "source": [
        "## Step 4: Run Training\n",
        "\n",
        "This final cell executes the training script. It will print the average reward for each epoch. Training will take a while!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1d41266",
        "outputId": "bc5f75a5-dfc8-4ce3-878c-10e750e3d6fd"
      },
      "source": [
        "!python train_ppo.py"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Initializing Go2 RL Environment...\n",
            "/usr/local/lib/python3.12/dist-packages/gymnasium/spaces/box.py:236: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/gymnasium/spaces/box.py:306: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
            "  gym.logger.warn(\n",
            "Observation space dim: 38\n",
            "Action space dim: 12\n",
            "Starting PPO Training...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/train_ppo.py\", line 244, in <module>\n",
            "    main()\n",
            "  File \"/content/train_ppo.py\", line 134, in main\n",
            "    action, logprob, _, value = agent.get_action_and_value(next_obs.unsqueeze(0))\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/train_ppo.py\", line 69, in get_action_and_value\n",
            "    action = self.action_bias + self.action_scale * action_tanh\n",
            "                                ~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~\n",
            "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n"
          ]
        }
      ]
    }
  ]
}